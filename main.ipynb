{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "373e093c",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This project explores visual concept organization in the PatternMind dataset which contains 25,557 images distributed across 233 categories. Our primary objective is understanding and organization rather than maximizing classification accuracy: we aim to uncover patterns, clusters and relationships among visual categories in feature space.\n",
    "\n",
    "Our framework proceeds in four stages: (1) we train a custom CNN to extract 256-dimensional feature embeddings that capture the visual essence of each image; (2) we apply PCA for dimensionality reduction, retaining 85% of variance in 50 components; (3) we use hierarchical clustering on category centroids to discover data-driven macro-categories that reveal natural visual themes without predefined semantic labels; (4) we evaluate K-Means clustering against both fine-grained (233) and macro-category labels using Purity, ARI, and NMI metrics to assess unsupervised organization quality at different granularities.\n",
    "\n",
    "To establish performance baselines we also compare three supervised classifiers (Logistic Regression, SVM, and Random Forest) on the learned features. Our key findings demonstrate that CNN features encode meaningful visual structure: hierarchical clustering reveals coherent groupings such as space objects (saturn, mars, comet), birds (duck, goose, hummingbird), and vehicles, while simple linear classifiers achieve 40.9% accuracy on 233 classes, far above the 0.43% random baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66126d12",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Initial Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72c16aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_rel\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras import regularizers\n",
    "from PIL import Image\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import plotly.figure_factory as ff\n",
    "from plotly.subplots import make_subplots\n",
    "import keras\n",
    "from collections import Counter\n",
    "from keras import layers\n",
    "import tensorflow as tf\n",
    "from tensorflow.data import AUTOTUNE\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy.cluster.hierarchy import linkage, fcluster, leaves_list\n",
    "from scipy.spatial.distance import pdist, squareform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c9e724",
   "metadata": {},
   "source": [
    "### 1.1 Load Image Paths and Labels\n",
    "\n",
    "We load all images from the dataset directory where each subfolder represents a distinct category. The code below creates a DataFrame containing the file path, category name, and numeric label for each image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1dcf35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"db/patternmind_dataset/\"\n",
    "\n",
    "# Get all category folders (each subfolder is a class)\n",
    "categories = sorted([d for d in os.listdir(dataset_path) if os.path.isdir(\n",
    "    os.path.join(dataset_path, d)) and not d.startswith('.')])\n",
    "\n",
    "# Build a list of all images with their metadata\n",
    "data = []\n",
    "for label_id, category in enumerate(categories):\n",
    "    folder_path = os.path.join(dataset_path, category)\n",
    "    images = [f for f in os.listdir(\n",
    "        folder_path) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "\n",
    "    for img in images:\n",
    "        data.append({\n",
    "            'path': os.path.join(folder_path, img),\n",
    "            'category': category,\n",
    "            'label_id': label_id\n",
    "        })\n",
    "\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2706874",
   "metadata": {},
   "source": [
    "### 1.2 Dataset Statistics and Quality Check\n",
    "\n",
    "The code below checks for missing values, duplicates and computes summary statistics about the class distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f0d822",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values and data quality\n",
    "print(f\"Missing values in DataFrame: {df.isnull().sum().sum()}\")\n",
    "print(f\"Duplicate rows: {df.duplicated().sum()}\")\n",
    "\n",
    "# Detailed statistics\n",
    "counts = df['category'].value_counts()\n",
    "print(f\"\\nTotal images: {len(df)}\")\n",
    "print(f\"Total categories: {len(categories)}\")\n",
    "print(f\"Average images per category: {len(df) / len(categories):.2f}\")\n",
    "print(f\"Min images per category: {counts.min()}\")\n",
    "print(f\"Max images per category: {counts.max()}\")\n",
    "print(f\"Median images per category: {counts.median()}\")\n",
    "print(f\"Std deviation: {counts.std():.2f}\")\n",
    "print(\"\\nEdge cases (fewest images):\")\n",
    "print(counts.tail())\n",
    "print(\"\\nEdge cases (most images):\")\n",
    "print(counts.head())\n",
    "\n",
    "# Class imbalance analysis\n",
    "imbalance_ratio = counts.max() / counts.min()\n",
    "print(f\"\\nClass imbalance ratio (max/min): {imbalance_ratio:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d809c3fd",
   "metadata": {},
   "source": [
    "The output above shows:\n",
    "\n",
    "- No missing values or duplicates\n",
    "- 25557 total images across 233 categories\n",
    "- Average of ~110 images per category, but with significant variation\n",
    "- Class imbalance ratio of 10.57x between the most and least frequent categories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d743e60",
   "metadata": {},
   "source": [
    "## 2. Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77826bf4",
   "metadata": {},
   "source": [
    "### 2.1 Class Distribution Analysis\n",
    "\n",
    "We first examine how images are distributed across categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b707408a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_dist = px.bar(counts, x=counts.index, y=counts.values,\n",
    "                  title=\"Class Distribution\",\n",
    "                  labels={'index': 'Category', 'y': 'Count'})\n",
    "fig_dist.update_layout(xaxis={'categoryorder': 'total descending'})\n",
    "fig_dist.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1088bbde",
   "metadata": {},
   "source": [
    "The bar chart reveals significant class imbalance in the dataset:\n",
    "\n",
    "- The most frequent category (\"clutter\") has 761 images\n",
    "- The least frequent category (\"top-hat\") has only 72 images\n",
    "- This represents a 10x variation in class sizes\n",
    "\n",
    "This imbalance matters for both supervised classification and unsupervised clustering. During CNN training (Section 3) we use data augmentation to help the model generalize across all categories regardless of frequency. For clustering evaluation (Section 6) we use multiple metrics to assess different aspects of cluster quality: Adjusted Rand Index (ARI, which accounts for chance agreement and penalizes trivial solutions like assigning each point to its own cluster), and Normalized Mutual Information (NMI, which measures information overlap between the hierarchical macro-categories and K-Means clusterings)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51ddac6",
   "metadata": {},
   "source": [
    "### 2.2 Image Dimension Analysis\n",
    "\n",
    "Before feeding images to a neural network, we need to understand the variation in image sizes. Neural networks require fixed-size inputs, so we must choose an appropriate target resolution. We analyze a sample of 5,000 images to understand the distribution of widths and heights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5fb2173",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample images to analyze dimension variability\n",
    "sample_size = 5000\n",
    "sample_df = df.sample(n=sample_size)\n",
    "sizes = [Image.open(p).size for p in sample_df['path']]\n",
    "widths, heights = zip(*sizes)\n",
    "\n",
    "# Create side-by-side histograms for width and height distributions\n",
    "fig_sizes = make_subplots(rows=1, cols=2, subplot_titles=(\n",
    "    \"Width Distribution\", \"Height Distribution\"))\n",
    "fig_sizes.add_trace(go.Histogram(x=widths, name=\"Width\"), row=1, col=1)\n",
    "fig_sizes.add_trace(go.Histogram(x=heights, name=\"Height\"), row=1, col=2)\n",
    "fig_sizes.update_layout(\n",
    "    title_text=f\"Image Size Distribution (Sample n={sample_size})\")\n",
    "fig_sizes.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c0beda",
   "metadata": {},
   "source": [
    "The histograms show that image dimensions vary considerably:\n",
    "\n",
    "- Most images have widths and heights concentrated around 200-400 pixels\n",
    "- Some images are very small (thumbnails) while others are much larger\n",
    "\n",
    "We chose 224×224 resolution (the ImageNet standard) because it is large enough to preserve important visual details like edges, textures, and shapes, while remaining computationally efficient by reducing memory usage and training time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f56c2ba",
   "metadata": {},
   "source": [
    "### 2.3 Visual Sample Inspection\n",
    "\n",
    "To understand the visual diversity in our dataset we display random samples from five different categories. This helps us appreciate the challenges our models will face: varying backgrounds, lighting conditions, object orientations and image quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c06867e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_categories = np.random.choice(categories, 5, replace=False)\n",
    "fig_imgs = make_subplots(rows=1, cols=5, subplot_titles=sample_categories)\n",
    "\n",
    "for i, cat in enumerate(sample_categories):\n",
    "    img_path = df[df['category'] == cat].sample(1).iloc[0]['path']\n",
    "    img = Image.open(img_path)\n",
    "    img_array = np.array(img)\n",
    "    fig_imgs.add_trace(go.Image(z=img_array), row=1, col=i+1)\n",
    "\n",
    "fig_imgs.update_layout(title_text=\"Sample Images from Random Categories\")\n",
    "fig_imgs.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597e09fe",
   "metadata": {},
   "source": [
    "The random samples reveal substantial visual diversity within and across categories.\n",
    "\n",
    "To address this variability, we use data augmentation (random flips, rotations, zoom, translations) so the model sees different orientations and scales of the same objects. The CNN’s stacked convolutional layers then learn hierarchical features that stay robust to these variations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac11f38b",
   "metadata": {},
   "source": [
    "## 3. CNN Feature Extraction\n",
    "\n",
    "We use a CNN for feature extraction because these networks learn representations at multiple levels of complexity. Early layers detect simple patterns like edges and textures while deeper layers recognize more complex features like shapes and objects. The layer just before the final classification output contains a rich and compressed representation of the image that captures its essential visual characteristics. These learned features work much better for clustering than using raw pixel values directly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23934a9",
   "metadata": {},
   "source": [
    "### 3.1 Data Preprocessing and Augmentation\n",
    "\n",
    "Before training the CNN, we standardize inputs and apply light augmentation:\n",
    "\n",
    "- Resize to 224×224: fixed input shape\n",
    "- Normalize to [0, 1]: scale pixels for faster convergence\n",
    "- Random horizontal flip: handle left/right orientation\n",
    "- Random rotation (±0.2 rad): cover tilt\n",
    "- Random zoom (±20%): cover scale changes\n",
    "- Random translation (±15% height/width): cover shifts\n",
    "- Random contrast (±20%): handle lighting variations\n",
    "- Random brightness (±20%): handle exposure variations\n",
    "\n",
    "We split 80% for training and 20% for validation. In this project, we use the validation set both for early stopping during CNN training and for reporting final performance. For a production system requiring extensive hyperparameter tuning, a three-way split (train/validation/test) would prevent overfitting to the validation set. However, given that our CNN training takes approximately 5 hours and our primary objective is feature extraction for clustering rather than achieving peak classification accuracy, we trained the model only once with carefully chosen hyperparameters. This makes the two-way split acceptable for our use case, as we avoid the computational cost of multiple training runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a4b165",
   "metadata": {},
   "outputs": [],
   "source": [
    "# type: ignore\n",
    "IMG_SIZE = (224, 224)\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Data augmentation: random flips, rotations, zooms, translations, contrast, brightness\n",
    "data_augmentation = keras.Sequential([\n",
    "    layers.RandomFlip(\"horizontal\"),\n",
    "    layers.RandomRotation(0.2),\n",
    "    layers.RandomZoom(0.2, 0.2),\n",
    "    layers.RandomTranslation(0.15, 0.15),\n",
    "    layers.RandomContrast(0.2),\n",
    "    layers.RandomBrightness(0.2),\n",
    "], name='augmentation')\n",
    "\n",
    "# Load training and validation datasets (80/20 split)\n",
    "train_ds = keras.utils.image_dataset_from_directory(\n",
    "    dataset_path,\n",
    "    validation_split=0.2,\n",
    "    subset=\"training\",\n",
    "    seed=123,\n",
    "    image_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    label_mode='categorical'\n",
    ")\n",
    "\n",
    "val_ds = keras.utils.image_dataset_from_directory(\n",
    "    dataset_path,\n",
    "    validation_split=0.2,\n",
    "    subset=\"validation\",\n",
    "    seed=123,\n",
    "    image_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    label_mode='categorical'\n",
    ")\n",
    "\n",
    "# Apply augmentation to training data and normalize both datasets\n",
    "train_ds = train_ds.map(lambda x, y: (x / 255.0, y),\n",
    "                        num_parallel_calls=AUTOTUNE)\n",
    "train_ds = train_ds.cache()\n",
    "train_ds = train_ds.map(lambda x, y: (data_augmentation(x, training=True), y))\n",
    "train_ds = train_ds.shuffle(1000).prefetch(AUTOTUNE)\n",
    "\n",
    "val_ds = val_ds.map(lambda x, y: (x / 255.0, y), num_parallel_calls=AUTOTUNE)\n",
    "val_ds = val_ds.cache().prefetch(AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eecede11",
   "metadata": {},
   "source": [
    "### 3.2 CNN Model Architecture\n",
    "\n",
    "We design a custom CNN with three convolutional blocks to extract visual features from images.\n",
    "\n",
    "| Layer Type    | Details                                                              | Purpose                                        |\n",
    "| ------------- | -------------------------------------------------------------------- | ---------------------------------------------- |\n",
    "| Conv Block 1  | 32 filters × 2 layers, 3×3 kernel, SAME pad, L2=1e-4                 | Detect low-level features (edges, colors)      |\n",
    "| Conv Block 2  | 64 filters × 2 layers, 3×3 kernel, SAME pad, L2=1e-4 + Dropout 0.25  | Detect mid-level features (textures, patterns) |\n",
    "| Conv Block 3  | 128 filters × 2 layers, 3×3 kernel, SAME pad, L2=1e-4 + Dropout 0.25 | Detect high-level features (shapes, parts)     |\n",
    "| GlobalAvgPool |                                                                      | Aggregate spatial features                     |\n",
    "| Dense Layer   | 256 neurons, ReLU, L2=1e-4 + Dropout 0.5                             | Compact 256-D representation for clustering    |\n",
    "| Output Layer  | 233 neurons, softmax                                                 | Classify into one of 233 categories            |\n",
    "\n",
    "Key choices:\n",
    "\n",
    "- Activation: ReLU throughout convolutional and dense layers.\n",
    "- Padding: padding=\"same\" keeps spatial size, preserving edges.\n",
    "- Regularization: L2 weight decay 1e-4 on all Conv/Dense layers.\n",
    "- Pooling: MaxPooling halves spatial dimensions in each block; GlobalAveragePooling before Dense.\n",
    "- Dropout: 0.25 after pooling in blocks 2–3; 0.5 before the output layer.\n",
    "- Batch size: 32, balancing stability and memory.\n",
    "- Optimizer: Adam with learning rate 0.0005.\n",
    "- Loss: Categorical crossentropy.\n",
    "- Callbacks: Early stopping on val_loss (patience 5, restore best) and ReduceLROnPlateau (factor 0.5, patience 3, min_lr 1e-7).\n",
    "\n",
    "Our goal isn’t peak classification accuracy, we primarily use the 256-D Dense layer as the feature embedding for clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a48fbf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = 'out/models/cnn_feature_extractor_v3.keras'\n",
    "HISTORY_PATH = 'out/models/training_history_v3.npy'\n",
    "\n",
    "if os.path.exists(MODEL_PATH):\n",
    "    model = keras.models.load_model(MODEL_PATH)\n",
    "    # history_data = np.load(HISTORY_PATH, allow_pickle=True).item()\n",
    "    print(\"Model loaded successfully!\")\n",
    "else:\n",
    "    # Build CNN with 3 convolutional blocks (32, 64, 128 filters)\n",
    "    model = keras.Sequential([\n",
    "        layers.Input(shape=(IMG_SIZE[0], IMG_SIZE[1], 3)),\n",
    "\n",
    "        # Block 1\n",
    "        layers.Conv2D(32, (3, 3), activation='relu', padding='same',\n",
    "                      kernel_regularizer=regularizers.l2(1e-4)),\n",
    "        layers.Conv2D(32, (3, 3), activation='relu', padding='same',\n",
    "                      kernel_regularizer=regularizers.l2(1e-4)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "\n",
    "        # Block 2\n",
    "        layers.Conv2D(64, (3, 3), activation='relu', padding='same',\n",
    "                      kernel_regularizer=regularizers.l2(1e-4)),\n",
    "        layers.Conv2D(64, (3, 3), activation='relu', padding='same',\n",
    "                      kernel_regularizer=regularizers.l2(1e-4)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Dropout(0.25),\n",
    "\n",
    "        # Block 3\n",
    "        layers.Conv2D(128, (3, 3), activation='relu', padding='same',\n",
    "                      kernel_regularizer=regularizers.l2(1e-4)),\n",
    "        layers.Conv2D(128, (3, 3), activation='relu', padding='same',\n",
    "                      kernel_regularizer=regularizers.l2(1e-4)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Dropout(0.25),\n",
    "\n",
    "        layers.GlobalAveragePooling2D(),\n",
    "\n",
    "        layers.Dense(256, activation='relu', name='features',\n",
    "                     kernel_regularizer=regularizers.l2(1e-4)),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(len(categories), activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=0.0005),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    # Training callbacks for early stopping and learning rate reduction\n",
    "    callbacks = [\n",
    "        keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=5,\n",
    "            restore_best_weights=True\n",
    "        ),\n",
    "        keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=3,\n",
    "            min_lr=1e-7\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    history = model.fit(\n",
    "        train_ds,\n",
    "        epochs=50,\n",
    "        validation_data=val_ds,\n",
    "        callbacks=callbacks\n",
    "    )\n",
    "\n",
    "    # Save the trained model\n",
    "    os.makedirs('out/models', exist_ok=True)\n",
    "    model.save(MODEL_PATH)\n",
    "    print(f\"Model saved to {MODEL_PATH}\")\n",
    "\n",
    "    # Save training history\n",
    "    np.save(HISTORY_PATH, history.history)\n",
    "    print(f\"Training history saved to {HISTORY_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81042a49",
   "metadata": {},
   "source": [
    "### 3.3 Training Performance Visualization\n",
    "\n",
    "The code below creates two side-by-side plots showing how the model's accuracy and loss changed during training. We plot key epochs where we recorded metrics to show the overall training progression. The vertical line marks epoch 33, where the best validation loss occurred and whose weights were restored by early stopping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318ee022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training history from reported epochs\n",
    "epochs_reported = [1, 5, 10, 15, 20, 25, 30, 33, 37, 38]\n",
    "accuracy_reported = [0.0709, 0.1471, 0.2088, 0.2652,\n",
    "                     0.3246, 0.3562, 0.3810, 0.4091, 0.4306, 0.4326]\n",
    "loss_reported = [5.151, 4.347, 3.841, 3.498,\n",
    "                 3.111, 2.930, 2.778, 2.624, 2.501, 2.496]\n",
    "val_accuracy_reported = [0.0605, 0.1285, 0.1446, 0.2000,\n",
    "                         0.2618, 0.2487, 0.2657, 0.3023, 0.3033, 0.3009]\n",
    "val_loss_reported = [5.081, 4.558, 4.479, 3.997,\n",
    "                     3.656, 3.726, 3.689, 3.409, 3.445, 3.470]\n",
    "\n",
    "# Visualize training history\n",
    "fig_history = make_subplots(\n",
    "    rows=1, cols=2, subplot_titles=('Accuracy', 'Loss'))\n",
    "\n",
    "fig_history.add_trace(go.Scatter(x=epochs_reported, y=accuracy_reported, name='Train',\n",
    "                                 mode='lines+markers', line=dict(color='#2E86AB')), row=1, col=1)\n",
    "fig_history.add_trace(go.Scatter(x=epochs_reported, y=val_accuracy_reported, name='Val',\n",
    "                                 mode='lines+markers', line=dict(color='#A23B72')), row=1, col=1)\n",
    "\n",
    "fig_history.add_trace(go.Scatter(x=epochs_reported, y=loss_reported, name='Train',\n",
    "                                 mode='lines+markers', line=dict(color='#2E86AB'), showlegend=False), row=1, col=2)\n",
    "fig_history.add_trace(go.Scatter(x=epochs_reported, y=val_loss_reported, name='Val',\n",
    "                                 mode='lines+markers', line=dict(color='#A23B72'), showlegend=False), row=1, col=2)\n",
    "\n",
    "fig_history.add_vline(x=33, line_dash=\"dash\", line_color=\"green\",\n",
    "                      opacity=0.5, annotation_text=\"Best (Epoch 33)\")\n",
    "\n",
    "fig_history.update_xaxes(title_text=\"Epoch\")\n",
    "fig_history.update_yaxes(title_text=\"Accuracy\", row=1, col=1)\n",
    "fig_history.update_yaxes(title_text=\"Loss\", row=1, col=2)\n",
    "fig_history.update_layout(title='CNN Training History', height=500, width=1000)\n",
    "fig_history.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45621f6a",
   "metadata": {},
   "source": [
    "| Epoch | Accuracy | Loss  | Val Accuracy | Val Loss | Learning Rate |\n",
    "| ----- | -------- | ----- | ------------ | -------- | ------------- |\n",
    "| 1     | 7.09%    | 5.151 | 6.05%        | 5.081    | 0.00050       |\n",
    "| 5     | 14.71%   | 4.347 | 12.85%       | 4.558    | 0.00050       |\n",
    "| 10    | 20.88%   | 3.841 | 14.46%       | 4.479    | 0.00050       |\n",
    "| 15    | 26.52%   | 3.498 | 20.00%       | 3.997    | 0.00050       |\n",
    "| 20    | 32.46%   | 3.111 | 26.18%       | 3.656    | 0.00025       |\n",
    "| 25    | 35.62%   | 2.930 | 24.87%       | 3.726    | 0.00025       |\n",
    "| 30    | 38.10%   | 2.778 | 26.57%       | 3.689    | 0.00025       |\n",
    "| 33    | 40.91%   | 2.624 | 30.23%       | 3.409    | 0.000125      |\n",
    "| 37    | 43.06%   | 2.501 | 30.33%       | 3.445    | 0.0000625     |\n",
    "| 38    | 43.26%   | 2.496 | 30.09%       | 3.470    | 0.0000625     |\n",
    "\n",
    "The model trained for 38 epochs before early stopping triggered (patience=5). The best validation loss of 3.41 occurred at epoch 33, and restore_best_weights=True restored those weights. While epoch 37 achieved the highest validation accuracy (30.33%), early stopping optimizes for loss, not accuracy.\n",
    "\n",
    "Looking at performance, the model hit its best validation accuracy of 30.3% at epoch 37 and its lowest validation loss of 3.41 at epoch 33. By the final epoch, training accuracy reached 43.3% while validation sat at 30.1%: a gap of about 13 percentage points. This gap shows the model is overfitting somewhat.\n",
    "\n",
    "That said, 30% validation accuracy on 233 categories isn't terrible (random guessing would only get 0.43%). More importantly, we're not really after high classification scores here. What matters is that the network learned useful visual patterns in those intermediate layers and the 256-dimensional feature embedding it produces is what we'll actually use for clustering downstream."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88bf5e9",
   "metadata": {},
   "source": [
    "### 3.4 Feature Extraction from All Images\n",
    "\n",
    "Now we extract 256-dimensional feature vectors from every image in the dataset. We do this by:\n",
    "\n",
    "1. Removing the final classification layer from the trained CNN\n",
    "2. Passing each image through the network\n",
    "3. Collecting the output from the Dense(256) layer\n",
    "\n",
    "These features capture the \"essence\" of each image as learned by the CNN: a compact representation that preserves visual similarity. Images that look alike will have similar feature vectors making them suitable for clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c838a480",
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES_PATH = 'out/features/features_v3.npy'\n",
    "LABELS_PATH = 'out/features/labels_v3.npy'\n",
    "\n",
    "if os.path.exists(FEATURES_PATH) and os.path.exists(LABELS_PATH):\n",
    "    features = np.load(FEATURES_PATH)\n",
    "    labels = np.load(LABELS_PATH)\n",
    "    print(f\"Features loaded: {features.shape}\")\n",
    "    print(f\"Labels loaded: {labels.shape}\")\n",
    "else:\n",
    "    # Create feature extractor by removing final classification layer\n",
    "    feature_extractor = keras.Sequential(model.layers[:-1])  # type: ignore\n",
    "\n",
    "    # Load all images without augmentation\n",
    "    all_ds = keras.utils.image_dataset_from_directory(\n",
    "        dataset_path,\n",
    "        label_mode='int',\n",
    "        shuffle=False,\n",
    "        image_size=IMG_SIZE,\n",
    "        batch_size=BATCH_SIZE\n",
    "    )\n",
    "\n",
    "    all_ds = all_ds.map(lambda x, y: (x / 255.0, y))  # type: ignore\n",
    "\n",
    "    # Extract 256-dimensional features for all images\n",
    "    features = feature_extractor.predict(all_ds.map(lambda x, y: x))\n",
    "    labels = np.concatenate([y.numpy() for _, y in all_ds], axis=0)\n",
    "\n",
    "    # Save features and labels\n",
    "    os.makedirs('out/features', exist_ok=True)\n",
    "    np.save(FEATURES_PATH, features)\n",
    "    np.save(LABELS_PATH, labels)\n",
    "    print(f\"Features extracted and saved:\")\n",
    "    print(f\"Features shape: {features.shape}\")\n",
    "    print(f\"Labels shape: {labels.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b95990",
   "metadata": {},
   "source": [
    "## 4. Dimensionality Reduction with PCA\n",
    "\n",
    "Our CNN outputs 256-dimensional feature vectors for each image, but working with that many dimensions has drawbacks. In high-dimensional spaces clustering algorithms slow down and visualizations become impossible. Plus, the CNN's 256 features likely contain redundancy: some dimensions might encode similar information or just noise.\n",
    "\n",
    "Principal Component Analysis (PCA) helps by finding new axes (principal components) that capture the maximum variance in our data. We can then keep only the most important components, throwing away redundant or noisy dimensions while preserving the essential structure. This makes clustering faster and more effective and lets us create 2D visualizations to understand what the CNN learned."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e678a56c",
   "metadata": {},
   "source": [
    "### 4.1 Feature Standardization and Variance Analysis\n",
    "\n",
    "We standardize all 256 CNN features to zero mean and unit variance then fit PCA to analyze how variance is distributed across components. This tells us how many dimensions actually carry useful information versus redundancy or noise. The cumulative variance plot below guides our choice of how many components to retain for downstream clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c52561",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_loaded = np.load(FEATURES_PATH)\n",
    "labels_loaded = np.load(LABELS_PATH)\n",
    "\n",
    "# Standardize features (mean=0, std=1)\n",
    "scaler = StandardScaler()\n",
    "features_scaled = scaler.fit_transform(features_loaded)\n",
    "\n",
    "# Fit PCA to analyze variance distribution\n",
    "pca_full = PCA()\n",
    "pca_full.fit(features_scaled)\n",
    "\n",
    "explained_variance = pca_full.explained_variance_ratio_\n",
    "cumulative_variance = np.cumsum(explained_variance)\n",
    "\n",
    "fig_variance = go.Figure()\n",
    "fig_variance.add_trace(go.Scatter(\n",
    "    x=list(range(1, len(explained_variance) + 1)),\n",
    "    y=cumulative_variance,\n",
    "    mode='lines+markers',\n",
    "    name='Cumulative Variance'\n",
    "))\n",
    "fig_variance.update_layout(\n",
    "    title='PCA Explained Variance',\n",
    "    xaxis_title='Number of Components',\n",
    "    yaxis_title='Cumulative Explained Variance',\n",
    "    hovermode='x'\n",
    ")\n",
    "fig_variance.show()\n",
    "\n",
    "print(\n",
    "    f\"Variance explained by first 2 components: {cumulative_variance[1]:.2%}\")\n",
    "print(\n",
    "    f\"Variance explained by first 10 components: {cumulative_variance[9]:.2%}\")\n",
    "print(\n",
    "    f\"Variance explained by first 50 components: {cumulative_variance[49]:.2%}\")\n",
    "print(\n",
    "    f\"Variance explained by first 60 components: {cumulative_variance[59]:.2%}\")\n",
    "print(\n",
    "    f\"Variance explained by first 75 components: {cumulative_variance[74]:.2%}\")\n",
    "print(\n",
    "    f\"Variance explained by first 100 components: {cumulative_variance[99]:.2%}\")\n",
    "print(\n",
    "    f\"Variance explained by first 120 components: {cumulative_variance[119]:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797642da",
   "metadata": {},
   "source": [
    "| Components | Variance Explained |\n",
    "| ---------- | ------------------ |\n",
    "| First 2    | 15.82%             |\n",
    "| First 10   | 50.51%             |\n",
    "| First 50   | 85.06%             |\n",
    "| First 60   | 87.31%             |\n",
    "| First 75   | 89.88%             |\n",
    "| First 100  | 92.95%             |\n",
    "| First 120  | 94.71%             |\n",
    "\n",
    "The variance distribution shows that the CNN's 256-dimensional output contains substantial redundancy.\n",
    "\n",
    "- The first 10 components alone capture over half the variance (50.51%)\n",
    "- The first 50 components retain 85% of the information\n",
    "- Beyond 100 components, variance gains diminish significantly\n",
    "- The curve shows smooth accumulation without a sharp elbow, indicating that variance is distributed across many components rather than concentrated in a few\n",
    "\n",
    "We choose 50 components for downstream clustering. This configuration retains 85.06% of the total variance while reducing dimensionality by over 80% compared to the original 256 dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38bf9245",
   "metadata": {},
   "source": [
    "### 4.2 2D Visualization\n",
    "\n",
    "While 50 dimensions is optimal for clustering, we cannot visualize 50D data directly. We project features to just 2 dimensions (PC1 and PC2) to create scatter plots that give us intuition about the data structure. We visualize 5 categories selected for their semantic diversity: airplanes, saturn, mars, duck and goose. Note that this 2D projection captures only ~16% of the total variance, so some overlap is expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633bdb14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce to 50D for clustering\n",
    "pca_50d = PCA(n_components=50)\n",
    "features_50d = pca_50d.fit_transform(features_scaled)\n",
    "\n",
    "# Reduce to 2D for visualization\n",
    "pca_2d = PCA(n_components=2)\n",
    "features_2d = pca_2d.fit_transform(features_scaled)\n",
    "\n",
    "# Visualize semantically distinct categories\n",
    "distinct_categories = ['airplanes', 'saturn', 'mars', 'duck', 'goose']\n",
    "distinct_indices = [i for i, cat in enumerate(\n",
    "    categories) if cat in distinct_categories]\n",
    "\n",
    "mask = np.isin(labels_loaded, distinct_indices)\n",
    "df_pca_distinct = pd.DataFrame({\n",
    "    'PC1': features_2d[mask, 0],\n",
    "    'PC2': features_2d[mask, 1],\n",
    "    'category': [categories[label] for label in labels_loaded[mask]]\n",
    "})\n",
    "\n",
    "fig_pca_distinct = px.scatter(\n",
    "    df_pca_distinct,\n",
    "    x='PC1',\n",
    "    y='PC2',\n",
    "    color='category',\n",
    "    title='PCA 2D Projection - Semantically Distinct Categories',\n",
    "    opacity=0.7\n",
    ")\n",
    "fig_pca_distinct.update_traces(marker=dict(size=5))\n",
    "fig_pca_distinct.update_layout(width=1000, height=800)\n",
    "fig_pca_distinct.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb14b50",
   "metadata": {},
   "source": [
    "The scatter plot reveals clear separation between semantically distinct categories. Airplanes (blue) form a tight, well-defined cluster in the upper-left region, demonstrating that the CNN learned distinctive features for this category.\n",
    "\n",
    "The space-related categories show interesting behavior: mars (purple) and saturn (orange) occupy the center region and overlap substantially with each other. This makes sense visually since both are celestial bodies with similar color palettes (warm oranges/reds) and spherical shapes. The CNN's features capture this shared \"space object\" signature.\n",
    "\n",
    "Similarly, the bird categories duck (orange) and goose (green) cluster together in the right portion of the plot, reflecting their shared visual characteristics: feathers, beaks, similar body shapes, and often similar backgrounds (water, grass).\n",
    "\n",
    "The key insight is that even in just 2 dimensions (capturing only ~16% of variance), the CNN features successfully separate semantically different concepts (vehicles vs. space objects vs. birds) while grouping visually similar categories together. This confirms that the learned representations encode meaningful visual structure that hierarchical and K-Means clustering can exploit in the full 50-dimensional space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7798b56",
   "metadata": {},
   "source": [
    "### 4.3 Data-Driven Macro-Category Discovery\n",
    "\n",
    "Rather than imposing predefined semantic groupings (e.g., \"animals,\" \"vehicles\"), we let the CNN features reveal natural visual clusters. We apply hierarchical clustering to the 233 category centroids using Ward linkage, which minimizes within-cluster variance at each merge step. The resulting dendrogram shows how categories progressively combine as we relax similarity thresholds.\n",
    "\n",
    "Categories merging at low heights are visually similar according to the CNN's learned representations, while those merging only at high heights are fundamentally different. By cutting the dendrogram at various heights, we can create different numbers of macro-categories (from 5 broad groups to 30+ finer divisions) for use in our clustering evaluation in Section 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2056d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute centroids for each of the 233 categories\n",
    "# A centroid is the mean feature vector of all images in that category\n",
    "category_centroids = []\n",
    "for i, category in enumerate(categories):\n",
    "    mask = labels_loaded == i\n",
    "    centroid = features_50d[mask].mean(axis=0)\n",
    "    category_centroids.append(centroid)\n",
    "\n",
    "category_centroids = np.array(category_centroids)\n",
    "\n",
    "# Perform hierarchical clustering on category centroids using Ward linkage\n",
    "# Ward linkage minimizes within-cluster variance at each merge step\n",
    "linkage_matrix = linkage(category_centroids, method='ward')\n",
    "\n",
    "color_threshold = 0.5 * max(linkage_matrix[:, 2])  # ~50% of max height\n",
    "\n",
    "fig_dendro_analysis = ff.create_dendrogram(\n",
    "    category_centroids,\n",
    "    labels=categories,\n",
    "    linkagefun=lambda x: linkage_matrix,\n",
    "    color_threshold=color_threshold\n",
    ")\n",
    "\n",
    "# Add horizontal line showing a suggested cut threshold\n",
    "fig_dendro_analysis.add_hline(\n",
    "    y=color_threshold,\n",
    "    line_dash=\"dash\",\n",
    "    line_color=\"red\",\n",
    "    annotation_text=f\"Cut threshold ({color_threshold:.1f})\",\n",
    "    annotation_position=\"top right\"\n",
    ")\n",
    "\n",
    "fig_dendro_analysis.update_layout(\n",
    "    title='Hierarchical Clustering Dendrogram of 233 Category Centroids<br>(Ward Linkage, 50D PCA Features)',\n",
    "    xaxis_title='Category',\n",
    "    yaxis_title='Ward Distance',\n",
    "    height=800,\n",
    "    width=1400,\n",
    "    xaxis={'tickangle': 90, 'tickfont': {'size': 6}}\n",
    ")\n",
    "\n",
    "fig_dendro_analysis.show()\n",
    "\n",
    "# Analyze the structure at different cut heights\n",
    "print(\"Macro-category counts at different cut heights:\")\n",
    "cut_heights = [0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "max_height = max(linkage_matrix[:, 2])\n",
    "\n",
    "for ratio in cut_heights:\n",
    "    cut_height = ratio * max_height\n",
    "    n_clusters = len(\n",
    "        set(fcluster(linkage_matrix, cut_height, criterion='distance')))\n",
    "    print(\n",
    "        f\"Cut at {ratio*100:.0f}% max height ({cut_height:.1f}): {n_clusters} clusters\")\n",
    "\n",
    "# Show example macro-categories at K=15 level\n",
    "n_macro_example = 15\n",
    "macro_assignments = fcluster(\n",
    "    linkage_matrix, n_macro_example, criterion='maxclust')\n",
    "\n",
    "# Group categories by their macro-category assignment\n",
    "macro_groups = {}\n",
    "for cat_idx, macro_id in enumerate(macro_assignments):\n",
    "    if macro_id not in macro_groups:\n",
    "        macro_groups[macro_id] = []\n",
    "    macro_groups[macro_id].append(categories[cat_idx])\n",
    "\n",
    "# Sort by group size and show the largest macro-categories\n",
    "sorted_groups = sorted(macro_groups.items(),\n",
    "                       key=lambda x: len(x[1]), reverse=True)\n",
    "\n",
    "# Find groups with 3-8 members (likely more coherent)\n",
    "coherent_groups = [(macro_id, members) for macro_id, members in sorted_groups\n",
    "                   if 3 <= len(members) <= 8]\n",
    "\n",
    "for macro_id, members in coherent_groups[:5]:\n",
    "    print(f\"\\nMacro-category {macro_id}: {', '.join(members)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a748f019",
   "metadata": {},
   "source": [
    "Our hierarchical clustering shows how the 233 categories naturally group into bigger visual families. By cutting the dendrogram at different heights, we can create different numbers of macro-categories: cutting at 50% of maximum height gives us 6 broad groups, while cutting at 30% gives us 19 smaller clusters. This lets us look at visual patterns at different levels of detail.\n",
    "\n",
    "When we look at 15 macro-categories, the CNN clearly organizes by visual features rather than semantic meaning. Macro-category 10 groups space-related images (comet, fireworks, galaxy, lightning, mars, rainbow, saturn) because they share common visual traits: radial patterns, glowing effects, dark backgrounds, and warm colors (oranges, yellows, purples). Macro-category 11 brings together organic subjects (butterfly, grapes, grasshopper, hibiscus, hummingbird, iris, praying-mantis, spider) based on detailed patterns, bright colors, curved shapes, and fine textures.\n",
    "\n",
    "However, Macro-category 12 shows an important weakness of visual-only clustering: it puts together conch shells, goldfish, hamburgers, hot dogs, ice cream cones, spaghetti, and teddy bears. These items have nothing in common semantically, but they look similar to the CNN: rounded shapes, warm colors (browns, yellows, reds), and compact layouts. This shows that unsupervised clustering groups by appearance in feature space, not by conceptual meaning. Despite this limitation, these results confirm that our CNN learned useful visual patterns for organizing large image collections into understandable groups."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110fd17a",
   "metadata": {},
   "source": [
    "### 4.4 Category Centroid Distance Heatmap\n",
    "\n",
    "To visualize how categories relate to each other in the 50D feature space we compute pairwise Euclidean distances between all 233 category centroids. The distance matrix is then reordered according to the hierarchical clustering leaf order, so that visually similar categories appear adjacent to each other. We display the first 100 categories to keep the heatmap readable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015c7645",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute pairwise Euclidean distances between all 233 category centroids\n",
    "centroid_distances = pdist(category_centroids, metric='euclidean')\n",
    "distance_matrix = squareform(centroid_distances)\n",
    "\n",
    "# Get the optimal leaf ordering from hierarchical clustering\n",
    "# This reorders categories so that similar ones are adjacent\n",
    "leaf_order = leaves_list(linkage_matrix)\n",
    "ordered_categories = [categories[i] for i in leaf_order]\n",
    "\n",
    "# Reorder the distance matrix according to clustering\n",
    "ordered_distance_matrix = distance_matrix[np.ix_(leaf_order, leaf_order)]\n",
    "\n",
    "# Show a zoomed version\n",
    "n_zoom = 100\n",
    "zoom_categories = ordered_categories[:n_zoom]\n",
    "\n",
    "fig_heatmap_zoom = go.Figure(data=go.Heatmap(\n",
    "    z=ordered_distance_matrix[:n_zoom, :n_zoom],\n",
    "    x=zoom_categories,\n",
    "    y=zoom_categories,\n",
    "    colorscale='Viridis',\n",
    "    reversescale=True,\n",
    "    colorbar=dict(title='Euclidean Distance'),\n",
    "    hovertemplate='%{x} ↔ %{y}<br>Distance: %{z:.3f}<extra></extra>'\n",
    "))\n",
    "\n",
    "fig_heatmap_zoom.update_layout(\n",
    "    title=f'Zoomed Heatmap: First {n_zoom} Categories (Clustered Order)',\n",
    "    xaxis=dict(tickangle=45, tickfont=dict(size=9)),\n",
    "    yaxis=dict(tickfont=dict(size=9), autorange='reversed'),\n",
    "    width=900,\n",
    "    height=800\n",
    ")\n",
    "\n",
    "fig_heatmap_zoom.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251501c3",
   "metadata": {},
   "source": [
    "The heatmap reveals the distance structure among the first 100 categories ordered by hierarchical clustering. The diagonal shows zero distance (yellow) as expected for self-comparisons. Several block structures are visible along the diagonal, indicating groups of categories with low inter-category distances (teal/green regions).\n",
    "\n",
    "Notable patterns include a cluster of electronic/household objects in the upper-left (ipod, breadmaker, photocopier, microwave) and another group of elongated objects mid-section (chopsticks, tuning-fork, eyeglasses). The bottom-right corner shows a distinct block containing architectural structures (skyscraper, minaret, pyramid) with relatively low distances to each other but high distances (purple) to most other categories.\n",
    "\n",
    "The predominant teal coloring across most of the matrix indicates moderate distances between categories, suggesting that while the CNN learned to distinguish broad visual concepts, many categories share enough visual features (shapes, textures, colors) to remain relatively close in feature space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd48be59",
   "metadata": {},
   "source": [
    "## 5. Supervised Baseline Classifier\n",
    "\n",
    "Before evaluating unsupervised clustering, we establish a supervised baseline to measure how well the CNN features support classification when ground-truth labels are available. This upper bound helps us contextualize the clustering results: the gap between supervised accuracy and unsupervised purity reveals how much information is lost when labels are unavailable.\n",
    "\n",
    "We use Logistic Regression because it's fast, interpretable, and effective with high-dimensional features. Logistic Regression learns simple linear decision boundaries in the 256-dimensional feature space. Strong performance here would confirm that the CNN successfully encoded discriminative visual information suitable for both classification and clustering tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9645c381",
   "metadata": {},
   "source": [
    "### 5.1 Train-Test Split with Stratification\n",
    "\n",
    "We split our data into 80% training and 20% test sets using stratified sampling. Stratification ensures that each category maintains the same proportion in both sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c96c758",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    features_scaled,\n",
    "    labels_loaded,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=labels_loaded\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {X_train.shape[0]}\")\n",
    "print(f\"Test set size: {X_test.shape[0]}\")\n",
    "print(f\"Feature dimensions: {X_train.shape[1]}\")\n",
    "print(f\"Number of classes: {len(np.unique(y_train))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd935e6",
   "metadata": {},
   "source": [
    "### 5.2 Train Logistic Regression Classifier\n",
    "\n",
    "We train multiple Logistic Regression configurations on the 256-dimensional CNN features to find the best balance between fitting the training data and generalizing to unseen examples:\n",
    "\n",
    "| Configuration    | Description                                                   |\n",
    "| ---------------- | ------------------------------------------------------------- |\n",
    "| Baseline (C=1.0) | Default regularization strength                               |\n",
    "| C=0.1            | Stronger L2 regularization to reduce overfitting              |\n",
    "| C=0.1 + balanced | Regularization with class weighting for imbalanced categories |\n",
    "| PCA-50D + C=0.1  | Regularization on dimensionality-reduced features             |\n",
    "\n",
    "Key settings:\n",
    "\n",
    "- max_iter=1000: Sufficient iterations for convergence with 233 classes\n",
    "- solver='lbfgs': Efficient optimization for multinomial classification\n",
    "- C parameter: Controls regularization strength (lower = stronger regularization)\n",
    "- n_jobs=-1: Parallel processing across all CPU cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac81b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# type: ignore\n",
    "\n",
    "# Baseline: default Logistic Regression\n",
    "lr_baseline = LogisticRegression(\n",
    "    max_iter=1000,\n",
    "    random_state=42,\n",
    "    solver='lbfgs',\n",
    "    n_jobs=-1\n",
    ")\n",
    "lr_baseline.fit(X_train, y_train)\n",
    "\n",
    "# Approach 1: Stronger regularization (C=0.1)\n",
    "lr_regularized = LogisticRegression(\n",
    "    max_iter=1000,\n",
    "    random_state=42,\n",
    "    solver='lbfgs',\n",
    "    C=0.1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "lr_regularized.fit(X_train, y_train)\n",
    "\n",
    "# Approach 2: Regularization + class balancing\n",
    "lr_balanced = LogisticRegression(\n",
    "    max_iter=1000,\n",
    "    random_state=42,\n",
    "    solver='lbfgs',\n",
    "    C=0.1,\n",
    "    class_weight='balanced',\n",
    "    n_jobs=-1\n",
    ")\n",
    "lr_balanced.fit(X_train, y_train)\n",
    "\n",
    "# Approach 3: PCA-reduced features (50D)\n",
    "X_train_pca, X_test_pca, y_train_pca, y_test_pca = train_test_split(\n",
    "    features_50d, labels_loaded, test_size=0.2, random_state=42, stratify=labels_loaded\n",
    ")\n",
    "lr_pca = LogisticRegression(\n",
    "    max_iter=1000,\n",
    "    random_state=42,\n",
    "    solver='lbfgs',\n",
    "    C=0.1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "lr_pca.fit(X_train_pca, y_train_pca)\n",
    "\n",
    "# Collect results\n",
    "results = []\n",
    "configs = [\n",
    "    (\"Baseline (C=1.0)\", lr_baseline, X_train, X_test, y_train, y_test),\n",
    "    (\"C=0.1\", lr_regularized, X_train, X_test, y_train, y_test),\n",
    "    (\"C=0.1 + balanced\", lr_balanced, X_train, X_test, y_train, y_test),\n",
    "    (\"PCA-50D + C=0.1\", lr_pca, X_train_pca, X_test_pca, y_train_pca, y_test_pca),\n",
    "]\n",
    "\n",
    "for name, model, X_tr, X_te, y_tr, y_te in configs:\n",
    "    train_acc = accuracy_score(y_tr, model.predict(X_tr))\n",
    "    test_acc = accuracy_score(y_te, model.predict(X_te))\n",
    "    y_pred = model.predict(X_te)\n",
    "    p_macro, r_macro, f1_macro, _ = precision_recall_fscore_support(\n",
    "        y_te, y_pred, average='macro', zero_division=0\n",
    "    )\n",
    "    p_weighted, r_weighted, f1_weighted, _ = precision_recall_fscore_support(\n",
    "        y_te, y_pred, average='weighted', zero_division=0\n",
    "    )\n",
    "    results.append({\n",
    "        'Configuration': name,\n",
    "        'Train Acc': train_acc,\n",
    "        'Test Acc': test_acc,\n",
    "        'Macro F1': f1_macro,\n",
    "        'Weighted F1': f1_weighted\n",
    "    })\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "print(df_results.to_string(index=False))\n",
    "\n",
    "# Select best model for downstream analysis\n",
    "best_idx = df_results['Test Acc'].idxmax()\n",
    "best_config = df_results.loc[best_idx, 'Configuration']\n",
    "\n",
    "# Use best model for predictions\n",
    "if 'PCA' in best_config:\n",
    "    lr_classifier = lr_pca\n",
    "    y_test_pred = lr_pca.predict(X_test_pca)\n",
    "    y_train_pred = lr_pca.predict(X_train_pca)\n",
    "    train_accuracy = accuracy_score(y_train_pca, y_train_pred)\n",
    "    test_accuracy = accuracy_score(y_test_pca, y_test_pred)\n",
    "elif 'balanced' in best_config:\n",
    "    lr_classifier = lr_balanced\n",
    "    y_test_pred = lr_balanced.predict(X_test)\n",
    "    y_train_pred = lr_balanced.predict(X_train)\n",
    "    train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "    test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "elif 'C=0.1' in best_config:\n",
    "    lr_classifier = lr_regularized\n",
    "    y_test_pred = lr_regularized.predict(X_test)\n",
    "    y_train_pred = lr_regularized.predict(X_train)\n",
    "    train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "    test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "else:\n",
    "    lr_classifier = lr_baseline\n",
    "    y_test_pred = lr_baseline.predict(X_test)\n",
    "    y_train_pred = lr_baseline.predict(X_train)\n",
    "    train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "    test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "# Final metrics for best model\n",
    "precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(\n",
    "    y_test if 'PCA' not in best_config else y_test_pca,\n",
    "    y_test_pred, average='macro', zero_division=0\n",
    ")\n",
    "precision_weighted, recall_weighted, f1_weighted, _ = precision_recall_fscore_support(\n",
    "    y_test if 'PCA' not in best_config else y_test_pca,\n",
    "    y_test_pred, average='weighted', zero_division=0\n",
    ")\n",
    "\n",
    "print(f\"\\nBest Model: {best_config}\")\n",
    "print(f\"Training Accuracy: {train_accuracy:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"\\nMacro-averaged metrics:\")\n",
    "print(f\"  Precision: {precision_macro:.4f}\")\n",
    "print(f\"  Recall: {recall_macro:.4f}\")\n",
    "print(f\"  F1-Score: {f1_macro:.4f}\")\n",
    "print(f\"\\nWeighted-averaged metrics:\")\n",
    "print(f\"  Precision: {precision_weighted:.4f}\")\n",
    "print(f\"  Recall: {recall_weighted:.4f}\")\n",
    "print(f\"  F1-Score: {f1_weighted:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666ee9c6",
   "metadata": {},
   "source": [
    "| Configuration    | Train Acc | Test Acc | Gap   | Macro F1 | Weighted F1 |\n",
    "| ---------------- | --------- | -------- | ----- | -------- | ----------- |\n",
    "| Baseline (C=1.0) | 80.7%     | 37.7%    | 43.0% | 31.6%    | 37.3%       |\n",
    "| C=0.1            | 63.8%     | 40.9%    | 22.9% | 33.8%    | 39.4%       |\n",
    "| C=0.1 + balanced | 63.9%     | 39.2%    | 24.7% | 33.3%    | 38.7%       |\n",
    "| PCA-50D + C=0.1  | 46.7%     | 39.2%    | 7.5%  | 31.5%    | 37.1%       |\n",
    "\n",
    "Best Model: C=0.1 (strongest regularization on full 256D features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e629b1",
   "metadata": {},
   "source": [
    "### 5.2.1 Cross-Validation Analysis\n",
    "\n",
    "To obtain more robust performance estimates, we perform 5-fold stratified cross-validation on our best configuration (C=0.1). This tests the model on different data splits, reducing variance from a single train/test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3e94ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5-fold stratified cross-validation\n",
    "lr_cv = LogisticRegression(\n",
    "    max_iter=1000,\n",
    "    random_state=42,\n",
    "    solver='lbfgs',\n",
    "    C=0.1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "cv_results = cross_validate(\n",
    "    lr_cv,\n",
    "    features_scaled,\n",
    "    labels_loaded,\n",
    "    cv=5,\n",
    "    scoring=['accuracy', 'f1_macro', 'f1_weighted'],\n",
    "    n_jobs=-1,\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "# Compute statistics\n",
    "cv_stats = {\n",
    "    'Train Accuracy': {\n",
    "        'mean': cv_results['train_accuracy'].mean(),\n",
    "        'std': cv_results['train_accuracy'].std()\n",
    "    },\n",
    "    'Test Accuracy': {\n",
    "        'mean': cv_results['test_accuracy'].mean(),\n",
    "        'std': cv_results['test_accuracy'].std()\n",
    "    },\n",
    "    'Macro F1': {\n",
    "        'mean': cv_results['test_f1_macro'].mean(),\n",
    "        'std': cv_results['test_f1_macro'].std()\n",
    "    },\n",
    "    'Weighted F1': {\n",
    "        'mean': cv_results['test_f1_weighted'].mean(),\n",
    "        'std': cv_results['test_f1_weighted'].std()\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"5-Fold Cross-Validation Results (C=0.1):\")\n",
    "print(\n",
    "    f\"Test Accuracy:  {cv_stats['Test Accuracy']['mean']:.4f} ± {cv_stats['Test Accuracy']['std']:.4f}\")\n",
    "print(\n",
    "    f\"Train Accuracy: {cv_stats['Train Accuracy']['mean']:.4f} ± {cv_stats['Train Accuracy']['std']:.4f}\")\n",
    "print(\n",
    "    f\"Macro F1:       {cv_stats['Macro F1']['mean']:.4f} ± {cv_stats['Macro F1']['std']:.4f}\")\n",
    "print(\n",
    "    f\"Weighted F1:    {cv_stats['Weighted F1']['mean']:.4f} ± {cv_stats['Weighted F1']['std']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc22fd88",
   "metadata": {},
   "source": [
    "| Metric         | Mean   | Std Dev  |\n",
    "| -------------- | ------ | -------- |\n",
    "| Train Accuracy | 0.6375 | ± 0.0032 |\n",
    "| Test Accuracy  | 0.4042 | ± 0.0039 |\n",
    "| Macro F1       | 0.3356 | ± 0.0033 |\n",
    "| Weighted F1    | 0.3893 | ± 0.0041 |\n",
    "\n",
    "The 5-fold cross-validation confirms the robustness of our Logistic Regression classifier with strong regularization (C=0.1), achieving a mean test accuracy of 40.42% ± 0.39% across different data splits. The low standard deviation (less than 0.5%) indicates that performance is stable and not heavily dependent on the particular train/test split. The percentage gap between train and test accuracy reveals moderate overfitting, but this is expected given the challenge of distinguishing 233 visually similar categories with limited training samples per class. The macro F1 score of 33.56% is notably lower than the weighted F1 of 38.93%, indicating that the classifier performs worse on smaller, less frequent categories compared to larger ones: a direct consequence of class imbalance in the dataset. These cross-validation results validate our single-split findings from Section 5.2, confirming that 40% test accuracy represents a reliable upper bound for supervised classification on this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3cf9d6f",
   "metadata": {},
   "source": [
    "### 5.3 Per-Category Performance Analysis\n",
    "\n",
    "We examine how classification accuracy varies across individual categories. This reveals which types of images are easiest or hardest to classify, and whether performance correlates with category size or visual distinctiveness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77fb2149",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate per-category accuracy\n",
    "per_class_accuracy = []\n",
    "for i, category in enumerate(categories):\n",
    "    mask = y_test == i\n",
    "    if mask.sum() > 0:\n",
    "        class_acc = (y_test_pred[mask] == i).sum() / mask.sum()\n",
    "        per_class_accuracy.append({\n",
    "            'category': category,\n",
    "            'label_id': i,\n",
    "            'accuracy': class_acc,\n",
    "            'test_samples': mask.sum()\n",
    "        })\n",
    "\n",
    "df_class_acc = pd.DataFrame(per_class_accuracy)\n",
    "df_class_acc_sorted = df_class_acc.sort_values('accuracy', ascending=False)\n",
    "\n",
    "print(\"\\nTop 10 Best Performing Categories:\")\n",
    "print(df_class_acc_sorted.head(10)[\n",
    "      ['category', 'accuracy', 'test_samples']].to_string(index=False))\n",
    "\n",
    "print(\"\\nTop 10 Worst Performing Categories:\")\n",
    "print(df_class_acc_sorted.tail(10)[\n",
    "      ['category', 'accuracy', 'test_samples']].to_string(index=False))\n",
    "\n",
    "print(f\"\\nMean per-class accuracy: {df_class_acc['accuracy'].mean():.4f}\")\n",
    "print(f\"Median per-class accuracy: {df_class_acc['accuracy'].median():.4f}\")\n",
    "print(f\"Std per-class accuracy: {df_class_acc['accuracy'].std():.4f}\")\n",
    "\n",
    "fig_acc_dist = go.Figure()\n",
    "fig_acc_dist.add_trace(go.Histogram(\n",
    "    x=df_class_acc['accuracy'],\n",
    "    nbinsx=30,\n",
    "    name='Per-Class Accuracy'\n",
    "))\n",
    "fig_acc_dist.update_layout(\n",
    "    title='Distribution of Per-Class Accuracy',\n",
    "    xaxis_title='Accuracy',\n",
    "    yaxis_title='Number of Classes',\n",
    "    showlegend=False,\n",
    "    height=500\n",
    ")\n",
    "fig_acc_dist.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f8f739",
   "metadata": {},
   "source": [
    "| Best Categories | Accuracy | Test Samples | Worst Categories | Accuracy | Test Samples |\n",
    "| --------------- | -------- | ------------ | ---------------- | -------- | ------------ |\n",
    "| car-side        | 100.0%   | 21           | windmill         | 0%       | 17           |\n",
    "| leopards        | 100.0%   | 35           | tuning-fork      | 0%       | 18           |\n",
    "| faces-easy      | 98.7%    | 79           | rifle            | 0%       | 19           |\n",
    "| motorbikes      | 98.6%    | 144          | dog              | 0%       | 19           |\n",
    "| airplanes       | 97.9%    | 144          | mailbox          | 0%       | 17           |\n",
    "\n",
    "Most categories cluster around 20-40% accuracy while a small subset achieves 80%+ accuracy. Two categories (car-side, leopards) achieve perfect 100% classification and several others exceed 90% (faces-easy, motorbikes, airplanes, sunflower, mars). These high-performing categories share distinctive visual signatures: consistent shapes, unique textures, or characteristic compositions that make them easily separable in feature space.\n",
    "\n",
    "Categories that fail completely (0% accuracy) include everyday objects that appear in varied contexts (dog, goat, sneaker) or items with ambiguous visual features (tuning-fork, stirrups, spoon). These objects lack the consistent visual patterns needed for reliable classification.\n",
    "\n",
    "| Statistic                 | Value  |\n",
    "| ------------------------- | ------ |\n",
    "| Mean per-class accuracy   | 34.52% |\n",
    "| Median per-class accuracy | 30.43% |\n",
    "| Standard deviation        | 23.90% |\n",
    "\n",
    "The high standard deviation (23.9%) confirms dramatic performance variation across categories."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12858990",
   "metadata": {},
   "source": [
    "### 5.3.1 Confusion Matrix Analysis\n",
    "\n",
    "To understand misclassification patterns, we visualize a confusion matrix for the 20 most frequent categories. This reveals which category pairs the classifier confuses most often, providing insight into visual similarity as perceived by the learned features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014f1b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select top 30 most frequent categories\n",
    "top_30_counts = df['category'].value_counts().head(30)\n",
    "top_30_categories = top_30_counts.index.tolist()\n",
    "top_30_indices = [categories.index(cat) for cat in top_30_categories]\n",
    "\n",
    "# Filter predictions and ground truth for top 30 categories\n",
    "mask_top30 = np.isin(y_test, top_30_indices)\n",
    "y_test_top30 = y_test[mask_top30]\n",
    "y_pred_top30 = y_test_pred[mask_top30]\n",
    "\n",
    "# Map to 0-29 range for confusion matrix\n",
    "index_mapping = {old_idx: new_idx for new_idx,\n",
    "                 old_idx in enumerate(top_30_indices)}\n",
    "\n",
    "# Only include predictions that are in top 30 (filter out misclassifications to other categories)\n",
    "valid_mask = np.isin(y_pred_top30, top_30_indices)\n",
    "y_test_top30_filtered = y_test_top30[valid_mask]\n",
    "y_pred_top30_filtered = y_pred_top30[valid_mask]\n",
    "\n",
    "y_test_top30_remapped = np.array(\n",
    "    [index_mapping[idx] for idx in y_test_top30_filtered])\n",
    "y_pred_top30_remapped = np.array(\n",
    "    [index_mapping[idx] for idx in y_pred_top30_filtered])\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(y_test_top30_remapped,\n",
    "                      y_pred_top30_remapped, labels=range(30))\n",
    "# Avoid division by zero\n",
    "cm_normalized = cm.astype('float') / (cm.sum(axis=1, keepdims=True) + 1e-10)\n",
    "\n",
    "fig_cm = go.Figure(data=go.Heatmap(\n",
    "    z=cm_normalized,\n",
    "    x=top_30_categories,\n",
    "    y=top_30_categories,\n",
    "    colorscale='Blues',\n",
    "    text=cm,\n",
    "    texttemplate='%{text}',\n",
    "    textfont={'size': 8},\n",
    "    hovertemplate='True: %{y}<br>Predicted: %{x}<br>Count: %{text}<br>Rate: %{z:.2%}<extra></extra>',\n",
    "    colorbar=dict(title='Normalized Rate')\n",
    "))\n",
    "\n",
    "fig_cm.update_layout(\n",
    "    title='Confusion Matrix: Top 30 Most Frequent Categories<br>(Normalized by Row)',\n",
    "    xaxis_title='Predicted Category',\n",
    "    yaxis_title='True Category',\n",
    "    xaxis={'tickangle': 45, 'tickfont': {'size': 8}},\n",
    "    yaxis={'tickfont': {'size': 8}, 'autorange': 'reversed'},\n",
    "    width=1000,\n",
    "    height=900\n",
    ")\n",
    "\n",
    "fig_cm.show()\n",
    "\n",
    "# Identify most common misclassifications within top 30\n",
    "misclass_pairs = []\n",
    "for i in range(len(top_30_categories)):\n",
    "    for j in range(len(top_30_categories)):\n",
    "        if i != j and cm[i, j] > 0:\n",
    "            misclass_pairs.append({\n",
    "                'true_category': top_30_categories[i],\n",
    "                'predicted_category': top_30_categories[j],\n",
    "                'count': cm[i, j],\n",
    "                'error_rate': cm_normalized[i, j]\n",
    "            })\n",
    "\n",
    "df_misclass = pd.DataFrame(misclass_pairs).sort_values(\n",
    "    'count', ascending=False)\n",
    "\n",
    "print(\"\\nTop 10 Most Common Misclassifications (within Top 30 categories):\")\n",
    "print(df_misclass.head(10)[\n",
    "      ['true_category', 'predicted_category', 'count', 'error_rate']].to_string(index=False))\n",
    "\n",
    "# Report how many predictions were outside top 30\n",
    "n_outside = (~valid_mask).sum()\n",
    "print(f\"\\nNote: {n_outside}/{len(y_pred_top30)} predictions ({100*n_outside/len(y_pred_top30):.1f}%) were for categories outside the top 30.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f13c1c",
   "metadata": {},
   "source": [
    "The confusion matrix and misclassification analysis reveal that the classifier struggles most with visually ambiguous objects and cluttered scenes. The dominant error pattern is objects being misclassified as \"clutter\" (ladder -> clutter at 26.3%, t-shirt -> clutter at 10.5%), indicating the model conflates objects in messy backgrounds with the clutter category itself. Mattress images frequently confuse the classifier, being misidentified as bathtub (15.4%) or t-shirt (11.5%), likely due to shared visual properties.\n",
    "\n",
    "Several misclassifications expose the CNN's reliance on shape over semantics: hot-tub -> hammock (12.5%) reflects curved/suspended forms, while people -> faces-easy (15.8%) shows focus on facial features rather than full-body context. The fact that 25.2% of predictions fell outside the top 30 categories entirely indicates the classifier often reaches for rare categories when uncertain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee96b7c0",
   "metadata": {},
   "source": [
    "### 5.4 Supervised Classification on Macro-Categories\n",
    "\n",
    "To further validate that the data-driven macro-categories represent meaningful visual groupings, we train a Logistic Regression classifier on macro-category labels instead of the original 233 fine-grained categories. If macro-categories capture coherent visual concepts, classification accuracy should improve significantly compared to the fine-grained baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d880d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Logistic Regression on macro-categories at different granularities\n",
    "macro_classification_results = []\n",
    "macro_granularities = [5, 10, 15, 20, 30]\n",
    "\n",
    "\n",
    "def get_macro_labels(linkage_matrix, n_macro, category_labels):\n",
    "    \"\"\"Map each image's category to its macro-category from dendrogram cut.\"\"\"\n",
    "    cat_to_macro = fcluster(linkage_matrix, n_macro, criterion='maxclust')\n",
    "    return np.array([cat_to_macro[cat] - 1 for cat in category_labels])\n",
    "\n",
    "\n",
    "for n_macro in macro_granularities:\n",
    "    # Get macro-category labels for all images\n",
    "    macro_labels_all = get_macro_labels(linkage_matrix, n_macro, labels_loaded)\n",
    "\n",
    "    # Split data with stratification on macro-labels\n",
    "    X_train_macro, X_test_macro, y_train_macro, y_test_macro = train_test_split(\n",
    "        features_scaled,\n",
    "        macro_labels_all,\n",
    "        test_size=0.2,\n",
    "        random_state=42,\n",
    "        stratify=macro_labels_all\n",
    "    )\n",
    "\n",
    "    # Train Logistic Regression with best config (C=0.1)\n",
    "    lr_macro = LogisticRegression(\n",
    "        max_iter=1000,\n",
    "        random_state=42,\n",
    "        solver='lbfgs',\n",
    "        C=0.1,  # Best configuration from Section 5.2\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    lr_macro.fit(X_train_macro, y_train_macro)\n",
    "\n",
    "    # Evaluate\n",
    "    train_acc = accuracy_score(y_train_macro, lr_macro.predict(X_train_macro))\n",
    "    test_acc = accuracy_score(y_test_macro, lr_macro.predict(X_test_macro))\n",
    "\n",
    "    macro_classification_results.append({\n",
    "        'n_macro_categories': n_macro,\n",
    "        'train_accuracy': train_acc,\n",
    "        'test_accuracy': test_acc,\n",
    "        'random_baseline': 1 / n_macro\n",
    "    })\n",
    "\n",
    "    print(\n",
    "        f\"Macro-categories: {n_macro:2d} | Train Acc: {train_acc:.4f} | Test Acc: {test_acc:.4f} | Random: {1/n_macro:.4f}\")\n",
    "\n",
    "df_macro_class = pd.DataFrame(macro_classification_results)\n",
    "\n",
    "# Visualize results\n",
    "fig_macro_class = go.Figure()\n",
    "fig_macro_class.add_trace(go.Scatter(\n",
    "    x=df_macro_class['n_macro_categories'],\n",
    "    y=df_macro_class['test_accuracy'],\n",
    "    mode='lines+markers',\n",
    "    name='Test Accuracy',\n",
    "    marker=dict(size=10)\n",
    "))\n",
    "fig_macro_class.add_trace(go.Scatter(\n",
    "    x=df_macro_class['n_macro_categories'],\n",
    "    y=df_macro_class['random_baseline'],\n",
    "    mode='lines+markers',\n",
    "    name='Random Baseline',\n",
    "    line=dict(dash='dash')\n",
    "))\n",
    "fig_macro_class.add_hline(\n",
    "    y=0.409,\n",
    "    line_dash=\"dot\",\n",
    "    line_color=\"red\",\n",
    "    annotation_text=\"Fine-grained (233 classes): 40.9%\",\n",
    "    annotation_position=\"top right\"\n",
    ")\n",
    "fig_macro_class.update_layout(\n",
    "    title='Logistic Regression Accuracy on Data-Driven Macro-Categories (C=0.1)',\n",
    "    xaxis_title='Number of Macro-Categories',\n",
    "    yaxis_title='Test Accuracy',\n",
    "    height=500,\n",
    "    width=800\n",
    ")\n",
    "fig_macro_class.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63e2e31",
   "metadata": {},
   "source": [
    "As we reduce the number of target macro-categories from 30 to 5 test accuracy increases from 54% to 72%, showing that the CNN features support progressively better classification as the task becomes less granular. Even with just 5 broad visual groups, the classifier achieves 72% accuracy demonstrating that hierarchical clustering successfully identified meaningful visual structure. At all granularities, test accuracy remains far above the random baseline confirming that the learned features encode genuine visual patterns rather than noise. Reducing to 5-10 macro-categories achieves higher accuracy, validating our hierarchical clustering approach and suggesting that for practical applications with limited labeled data, training classifiers on 5-15 macro-categories is more effective than attempting fine-grained 233-way classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af047fd8",
   "metadata": {},
   "source": [
    "### 5.5 Logistic Regression vs SVM vs Random Forest\n",
    "\n",
    "While Logistic Regression provides interpretable linear decision boundaries, other classifiers may capture different patterns in the CNN features. We compare three approaches:\n",
    "\n",
    "1. Logistic Regression (C=0.1): Our best linear model from Section 5.2\n",
    "2. Support Vector Machine (SVM): Finds optimal separating hyperplanes with kernel trick\n",
    "3. Random Forest: Ensemble of decision trees using bagging\n",
    "\n",
    "All models use the same 256D CNN features and train/test split for fair comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdeea676",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM with RBF kernel\n",
    "svm_classifier = SVC(\n",
    "    kernel='rbf',\n",
    "    C=1.0,\n",
    "    gamma='scale',\n",
    "    random_state=42\n",
    ")\n",
    "svm_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Random Forest\n",
    "rf_classifier = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=20,\n",
    "    min_samples_split=5,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Compare all three classifiers\n",
    "classifier_comparison = []\n",
    "for name, clf in [\n",
    "    (\"Logistic Regression (C=0.1)\", lr_regularized),\n",
    "    (\"SVM (RBF)\", svm_classifier),\n",
    "    (\"Random Forest\", rf_classifier)\n",
    "]:\n",
    "    train_pred = clf.predict(X_train)\n",
    "    test_pred = clf.predict(X_test)\n",
    "\n",
    "    train_acc = accuracy_score(y_train, train_pred)\n",
    "    test_acc = accuracy_score(y_test, test_pred)\n",
    "    gap = train_acc - test_acc\n",
    "\n",
    "    _, _, f1_macro, _ = precision_recall_fscore_support(\n",
    "        y_test, test_pred, average='macro', zero_division=0)\n",
    "    _, _, f1_weighted, _ = precision_recall_fscore_support(\n",
    "        y_test, test_pred, average='weighted', zero_division=0)\n",
    "\n",
    "    classifier_comparison.append({\n",
    "        'Classifier': name,\n",
    "        'Train Acc': train_acc,\n",
    "        'Test Acc': test_acc,\n",
    "        'Gap': gap,\n",
    "        'Macro F1': f1_macro,\n",
    "        'Weighted F1': f1_weighted\n",
    "    })\n",
    "\n",
    "df_comparison = pd.DataFrame(classifier_comparison)\n",
    "print(df_comparison.to_string(index=False))\n",
    "\n",
    "# Visualization\n",
    "fig_comparison = go.Figure()\n",
    "for metric in ['Train Acc', 'Test Acc', 'Macro F1']:\n",
    "    fig_comparison.add_trace(go.Bar(\n",
    "        name=metric,\n",
    "        x=[row['Classifier'] for row in classifier_comparison],\n",
    "        y=[row[metric] for row in classifier_comparison],\n",
    "        text=[f\"{row[metric]:.3f}\" for row in classifier_comparison],\n",
    "        textposition='auto'\n",
    "    ))\n",
    "\n",
    "fig_comparison.update_layout(\n",
    "    title='Classifier Performance Comparison on 256D CNN Features',\n",
    "    xaxis_title='Classifier',\n",
    "    yaxis_title='Score',\n",
    "    barmode='group',\n",
    "    height=500,\n",
    "    width=900,\n",
    "    yaxis=dict(range=[0, 1])\n",
    ")\n",
    "fig_comparison.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba89bcd",
   "metadata": {},
   "source": [
    "| Classifier                      | Train Acc | Test Acc  | Gap   | Macro F1  | Weighted F1 |\n",
    "| ------------------------------- | --------- | --------- | ----- | --------- | ----------- |\n",
    "| **Logistic Regression (C=0.1)** | 63.8%     | **40.9%** | 22.9% | **33.8%** | **39.4%**   |\n",
    "| SVM (RBF)                       | 61.0%     | 38.5%     | 22.5% | 30.9%     | 36.2%       |\n",
    "| Random Forest                   | 98.9%     | 33.2%     | 65.7% | 22.7%     | 27.6%       |\n",
    "\n",
    "The comparison reveals some surprising results about what works best for classifying CNN features. Logistic Regression, despite being the simplest of the three models, achieves the highest test accuracy at 40.9% and the best Macro F1 score at 33.8%. This tells us something important: the CNN has already done the hard work of transforming images into a feature space where simple linear decision boundaries work well.\n",
    "\n",
    "The SVM with an RBF kernel performs slightly worse than Logistic Regression, getting 38.5% test accuracy. The RBF kernel is designed to find curved decision boundaries by projecting data into higher dimensions, but it doesn't help here. This reinforces the idea that the 256-dimensional CNN features are already well-organized for classification. Adding the kernel's complexity doesn't improve results and actually hurts performance slightly.\n",
    "\n",
    "Random Forest shows a dramatic case of overfitting. It achieves nearly perfect training accuracy at 98.9% but crashes down to just 33.2% on the test set. With 233 categories and 256 features, the trees can find all sorts of splits that perfectly separate training data but represent noise rather than real visual relationships. These patterns don't hold up when we test on new images.\n",
    "\n",
    "The lesson here is that regularization matters more than model sophistication when working with CNN features. Logistic Regression's L2 penalty (controlled by C=0.1) prevents it from overfitting by keeping the model weights small and simple. Random Forest, despite being a powerful ensemble method, lacks comparable regularization and ends up learning patterns that are too specific to the training set.\n",
    "The CNN has already learned the complex non-linear transformations during its training, so downstream classifiers just need to draw decision boundaries in the resulting feature space. Adding another layer of complexity tends to introduce overfitting without providing better discriminative power."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bce789d",
   "metadata": {},
   "source": [
    "### 5.5.1 Statistical Significance Testing\n",
    "\n",
    "To verify that performance differences between classifiers are statistically significant rather than due to random variation, we run each model with 10 different random seeds and perform paired t-tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c608fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run each classifier 10 times with different seeds\n",
    "n_runs = 10\n",
    "seeds = [42, 123, 456, 789, 1011, 1213, 1415, 1617, 1819, 2021]\n",
    "\n",
    "classifier_runs = {\n",
    "    'Logistic Regression (C=0.1)': [],\n",
    "    'SVM (RBF)': [],\n",
    "    'Random Forest': []\n",
    "}\n",
    "\n",
    "print(\"Running classifiers with multiple seeds...\")\n",
    "for seed in seeds:\n",
    "    # Split with different seed\n",
    "    X_tr, X_te, y_tr, y_te = train_test_split(\n",
    "        features_scaled, labels_loaded,\n",
    "        test_size=0.2, random_state=seed, stratify=labels_loaded\n",
    "    )\n",
    "\n",
    "    # Logistic Regression\n",
    "    lr = LogisticRegression(max_iter=1000, random_state=seed,\n",
    "                            solver='lbfgs', C=0.1, n_jobs=-1)\n",
    "    lr.fit(X_tr, y_tr)\n",
    "    lr_acc = accuracy_score(y_te, lr.predict(X_te))\n",
    "    classifier_runs['Logistic Regression (C=0.1)'].append(lr_acc)\n",
    "\n",
    "    # SVM\n",
    "    svm = SVC(kernel='rbf', C=1.0, gamma='scale', random_state=seed)\n",
    "    svm.fit(X_tr, y_tr)\n",
    "    svm_acc = accuracy_score(y_te, svm.predict(X_te))\n",
    "    classifier_runs['SVM (RBF)'].append(svm_acc)\n",
    "\n",
    "    # Random Forest\n",
    "    rf = RandomForestClassifier(n_estimators=100, max_depth=20, min_samples_split=5,\n",
    "                                random_state=seed, n_jobs=-1)\n",
    "    rf.fit(X_tr, y_tr)\n",
    "    rf_acc = accuracy_score(y_te, rf.predict(X_te))\n",
    "    classifier_runs['Random Forest'].append(rf_acc)\n",
    "\n",
    "    print(f\"Seed {seed}: LR={lr_acc:.4f}, SVM={svm_acc:.4f}, RF={rf_acc:.4f}\")\n",
    "\n",
    "# Compute statistics\n",
    "stats_results = []\n",
    "for name, scores in classifier_runs.items():\n",
    "    stats_results.append({\n",
    "        'Classifier': name,\n",
    "        'Mean': np.mean(scores),\n",
    "        'Std': np.std(scores),\n",
    "        'Min': np.min(scores),\n",
    "        'Max': np.max(scores)\n",
    "    })\n",
    "\n",
    "df_stats = pd.DataFrame(stats_results)\n",
    "print(\"\\nStatistical Summary (10 runs):\")\n",
    "print(df_stats.to_string(index=False))\n",
    "\n",
    "# Paired t-tests\n",
    "print(\"\\nPaired T-Tests (two-tailed):\")\n",
    "lr_scores = classifier_runs['Logistic Regression (C=0.1)']\n",
    "svm_scores = classifier_runs['SVM (RBF)']\n",
    "rf_scores = classifier_runs['Random Forest']\n",
    "\n",
    "t_stat_lr_svm, p_val_lr_svm = ttest_rel(lr_scores, svm_scores)\n",
    "t_stat_lr_rf, p_val_lr_rf = ttest_rel(lr_scores, rf_scores)\n",
    "t_stat_svm_rf, p_val_svm_rf = ttest_rel(svm_scores, rf_scores)\n",
    "\n",
    "print(f\"LR vs SVM:  t={t_stat_lr_svm:.3f}, p={p_val_lr_svm:.4f}\")\n",
    "print(f\"LR vs RF:   t={t_stat_lr_rf:.3f}, p={p_val_lr_rf:.4f}\")\n",
    "print(f\"SVM vs RF:  t={t_stat_svm_rf:.3f}, p={p_val_svm_rf:.4f}\")\n",
    "\n",
    "# Visualize distributions\n",
    "fig_sig = go.Figure()\n",
    "for name, scores in classifier_runs.items():\n",
    "    fig_sig.add_trace(go.Box(\n",
    "        y=scores,\n",
    "        name=name,\n",
    "        boxmean='sd'\n",
    "    ))\n",
    "\n",
    "fig_sig.update_layout(\n",
    "    title='Test Accuracy Distribution Across 10 Random Seeds',\n",
    "    yaxis_title='Test Accuracy',\n",
    "    showlegend=True,\n",
    "    height=500,\n",
    "    width=900\n",
    ")\n",
    "fig_sig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e029be5c",
   "metadata": {},
   "source": [
    "The statistical significance testing across 10 different random seeds confirms that Logistic Regression (C=0.1) is the superior classifier for CNN features on this dataset. Logistic Regression achieves a mean test accuracy of 41.13% ± 0.44% across all runs, significantly outperforming both SVM (39.07% ± 0.59%) and Random Forest (33.18% ± 0.52%). The paired t-tests reveal that all performance differences are highly statistically significant (p < 0.001). The low standard deviations across all methods (< 0.6%) indicate that results are stable and reproducible regardless of the train/test split. These findings definitively establish that for CNN-derived features, simple linear classification with proper regularization outperforms more complex models: the CNN's convolutional layers have already learned the critical non-linear transformations, so downstream classifiers benefit more from regularization that prevents overfitting than from additional model complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fdb2e04",
   "metadata": {},
   "source": [
    "## 6. K-Means Clustering Analysis\n",
    "\n",
    "Having established data-driven macro-categories through hierarchical clustering (Section 4.3), we now evaluate how well K-Means clustering aligns with these visual groupings. K-Means partitions images into k clusters by minimizing within-cluster variance, assigning each image to the nearest centroid. We measure cluster quality using three complementary metrics: Purity (cluster homogeneity), ARI (Adjusted Rand Index, which accounts for chance agreement), and NMI (Normalized Mutual Information, which measures information overlap). By evaluating these metrics against both the original 233 fine-grained categories and our hierarchically-derived macro-categories (5-30 groups), we can assess whether K-Means captures meaningful visual structure at different levels of granularity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e377541b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run K-Means with different cluster counts and compute multiple metrics\n",
    "\n",
    "cluster_counts = [10, 20, 50, 100, 233]\n",
    "\n",
    "\n",
    "def calculate_purity(cluster_labels, true_labels):\n",
    "    correct = sum(\n",
    "        Counter(true_labels[cluster_labels == c]).most_common(1)[0][1]\n",
    "        for c in np.unique(cluster_labels)\n",
    "    )\n",
    "    return correct / len(cluster_labels)\n",
    "\n",
    "\n",
    "# Run K-Means and calculate all metrics for different configurations\n",
    "all_clustering_results = []\n",
    "for k in cluster_counts:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    cluster_labels = kmeans.fit_predict(features_50d)\n",
    "\n",
    "    # Metrics against fine-grained (233) categories\n",
    "    fine_purity = calculate_purity(cluster_labels, labels_loaded)\n",
    "    fine_ari = adjusted_rand_score(labels_loaded, cluster_labels)\n",
    "    fine_nmi = normalized_mutual_info_score(labels_loaded, cluster_labels)\n",
    "\n",
    "    # Metrics against macro-categories at different granularities\n",
    "    for n_macro in macro_granularities:\n",
    "        macro_labels = get_macro_labels(linkage_matrix, n_macro, labels_loaded)\n",
    "        macro_purity = calculate_purity(cluster_labels, macro_labels)\n",
    "        macro_ari = adjusted_rand_score(macro_labels, cluster_labels)\n",
    "        macro_nmi = normalized_mutual_info_score(macro_labels, cluster_labels)\n",
    "\n",
    "        all_clustering_results.append({\n",
    "            'n_kmeans_clusters': k,\n",
    "            'n_macro_categories': n_macro,\n",
    "            'fine_grained_purity': fine_purity,\n",
    "            'fine_grained_ari': fine_ari,\n",
    "            'fine_grained_nmi': fine_nmi,\n",
    "            'macro_purity': macro_purity,\n",
    "            'macro_ari': macro_ari,\n",
    "            'macro_nmi': macro_nmi\n",
    "        })\n",
    "\n",
    "df_clustering = pd.DataFrame(all_clustering_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b395d564",
   "metadata": {},
   "source": [
    "### Purity, ARI, and NMI\n",
    "\n",
    "We visualize K-Means clustering quality using three complementary metrics across different granularities. Three heatmaps show Purity, ARI, and NMI across all combinations of K-Means clusters (10, 20, 50, 100, 233) and target category granularities (5, 10, 15, 20, 30 macro-categories, plus the original 233 fine-grained categories).\n",
    "\n",
    "- Purity measures cluster homogeneity but artificially increases with K, making it unreliable for comparing different cluster counts\n",
    "- ARI (Adjusted Rand Index) corrects for chance agreement, where ARI=0 means random clustering and ARI>0 proves real structure\n",
    "- NMI (Normalized Mutual Information) quantifies information overlap between cluster assignments and true categories\n",
    "\n",
    "Higher values (green in heatmaps) indicate better alignment between K-Means clusters and target categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a960fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: Three heatmaps for Purity, ARI, and NMI across macro-categories\n",
    "fig_heatmaps = make_subplots(\n",
    "    rows=1, cols=3,\n",
    "    subplot_titles=('Purity', 'Adjusted Rand Index (ARI)',\n",
    "                    'Normalized Mutual Information (NMI)'),\n",
    "    horizontal_spacing=0.12\n",
    ")\n",
    "\n",
    "# Prepare data for all three heatmaps\n",
    "pivot_purity = df_clustering.pivot(index='n_kmeans_clusters',\n",
    "                                   columns='n_macro_categories', values='macro_purity')\n",
    "pivot_purity[233] = df_clustering.groupby('n_kmeans_clusters')[\n",
    "    'fine_grained_purity'].first()\n",
    "pivot_purity = pivot_purity.sort_index(axis=1, ascending=False)\n",
    "\n",
    "pivot_ari = df_clustering.pivot(index='n_kmeans_clusters',\n",
    "                                columns='n_macro_categories', values='macro_ari')\n",
    "pivot_ari[233] = df_clustering.groupby('n_kmeans_clusters')[\n",
    "    'fine_grained_ari'].first()\n",
    "pivot_ari = pivot_ari.sort_index(axis=1, ascending=False)\n",
    "\n",
    "pivot_nmi = df_clustering.pivot(index='n_kmeans_clusters',\n",
    "                                columns='n_macro_categories', values='macro_nmi')\n",
    "pivot_nmi[233] = df_clustering.groupby('n_kmeans_clusters')[\n",
    "    'fine_grained_nmi'].first()\n",
    "pivot_nmi = pivot_nmi.sort_index(axis=1, ascending=False)\n",
    "\n",
    "# Purity heatmap\n",
    "fig_heatmaps.add_trace(\n",
    "    go.Heatmap(\n",
    "        z=pivot_purity.values,\n",
    "        x=[str(c) for c in pivot_purity.columns],\n",
    "        y=[f'K={k}' for k in pivot_purity.index],\n",
    "        colorscale='RdYlGn',\n",
    "        text=np.round(pivot_purity.values, 3),\n",
    "        texttemplate='%{text}',\n",
    "        textfont={'size': 10},\n",
    "        colorbar=dict(title='Purity', x=0.29),\n",
    "        showscale=True\n",
    "    ),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# ARI heatmap\n",
    "fig_heatmaps.add_trace(\n",
    "    go.Heatmap(\n",
    "        z=pivot_ari.values,\n",
    "        x=[str(c) for c in pivot_ari.columns],\n",
    "        y=[f'K={k}' for k in pivot_ari.index],\n",
    "        colorscale='RdYlGn',\n",
    "        text=np.round(pivot_ari.values, 3),\n",
    "        texttemplate='%{text}',\n",
    "        textfont={'size': 10},\n",
    "        colorbar=dict(title='ARI', x=0.63),\n",
    "        showscale=True\n",
    "    ),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "# NMI heatmap\n",
    "fig_heatmaps.add_trace(\n",
    "    go.Heatmap(\n",
    "        z=pivot_nmi.values,\n",
    "        x=[str(c) for c in pivot_nmi.columns],\n",
    "        y=[f'K={k}' for k in pivot_nmi.index],\n",
    "        colorscale='RdYlGn',\n",
    "        text=np.round(pivot_nmi.values, 3),\n",
    "        texttemplate='%{text}',\n",
    "        textfont={'size': 10},\n",
    "        colorbar=dict(title='NMI', x=1.0),\n",
    "        showscale=True\n",
    "    ),\n",
    "    row=1, col=3\n",
    ")\n",
    "\n",
    "fig_heatmaps.update_xaxes(title_text='Target Categories', row=1, col=1)\n",
    "fig_heatmaps.update_xaxes(title_text='Target Categories', row=1, col=2)\n",
    "fig_heatmaps.update_xaxes(title_text='Target Categories', row=1, col=3)\n",
    "fig_heatmaps.update_yaxes(title_text='K-Means Clusters', row=1, col=1)\n",
    "\n",
    "fig_heatmaps.update_layout(\n",
    "    title_text='K-Means Clustering: Purity, ARI, and NMI Across Different Granularities',\n",
    "    height=500,\n",
    "    width=1400\n",
    ")\n",
    "fig_heatmaps.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a18107",
   "metadata": {},
   "source": [
    "The three heatmaps reveal how K-Means clustering performs differently depending on which metric we use and how we group the categories. Looking at purity (left), we see values climbing from 10.6% (K=10, 233 categories) all the way to 66.7% (K=233, 5 macro-categories). This diagonal pattern makes sense: purity goes up when we either create more clusters or reduce the number of target groups we're comparing against. Purity, however, has a built-in bias that makes it look better as K increases, even if the clusters aren't actually meaningful.\n",
    "\n",
    "The ARI heatmap (middle) tells a more honest story. The best ARI scores (~0.12-0.13) show up in the middle range, around 10-20 macro-categories, suggesting that's where K-Means structure actually matches our hierarchical groupings. Against the original 233 fine-grained categories, ARI stays below 0.10 for all K values, confirming that K-Means just can't recover those detailed distinctions.\n",
    "\n",
    "Finally, NMI (right) provides the most stable view: values increase smoothly from 0.16-0.19 at 5 macro-categories up to 0.34-0.41 at 233 categories. K-Means works great for broad categorization (hitting 57-67% purity on 5 macro-categories), but it can't match the 40.9% supervised accuracy we got on fine-grained labels. For real-world use, this means K-Means on CNN features is useful for organizing images into general groups but you'll need labeled data and supervised learning if you want to tell apart specific categories."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418238d9",
   "metadata": {},
   "source": [
    "## 7. Conclusions\n",
    "\n",
    "### Summary of Findings\n",
    "\n",
    "This project shows that CNN features combined with hierarchical clustering can organize large image collections without exhaustive labeling. Our CNN achieved 30.3% validation accuracy on 233 classes: modest, but sufficient to learn meaningful visual representations. The 256-dimensional embeddings capture genuine similarity: space objects cluster by radial patterns and glowing effects, birds group by feather textures and body shapes, and vehicles form coherent structural clusters.\n",
    "\n",
    "Supervised classification confirms the features are discriminative. Logistic Regression achieved 40.9% test accuracy, outperforming SVM (38.5%) and Random Forest (33.2%). The CNN already learned the necessary non-linear transformations, so simple regularized classifiers work best. When classifying into 5 macro-categories instead of 233 fine-grained labels, accuracy jumped to 72%, validating that our hierarchical groupings capture real visual coherence.\n",
    "\n",
    "K-Means clustering achieved 57-67% purity on broad categories but struggled with fine-grained distinctions (ARI < 0.10 for 233 classes). Unsupervised methods work well for coarse organization but cannot replace supervised learning for detailed categorization.\n",
    "\n",
    "### Limitations\n",
    "\n",
    "Three main limitations constrain this work. First, our CNN is shallow compared to ResNet or EfficientNet and we trained from scratch rather than using ImageNet pretrained weights. This was intentional: to see what patterns a CNN learns from our data alone, but limits feature quality. Second, our evaluation metrics assume ground-truth labels are correct, yet PatternMind categories mix semantic concepts with visual properties inconsistently. The \"clutter\" category exemplifies this problem: it contains images of messy, disorganized scenes, but our confusion matrix (Section 5.3.1) shows that objects photographed against busy backgrounds (ladders, t-shirts) frequently get misclassified as clutter. The CNN cannot distinguish between an inherently cluttered scene and a clean object in a cluttered context: both produce similar visual features (complex textures, varied colors, lack of clear focal point). This label ambiguity propagates through our entire evaluation pipeline, inflating error rates on categories that happen to appear in visually noisy contexts. Third, CNN features optimized for classification don't form geometrically compact clusters, causing substantial overlap between categories in feature space.\n",
    "\n",
    "### Future Work\n",
    "\n",
    "Two directions would most improve this work. Transfer learning with a pretrained backbone (ResNet-50 or Vision Transformer) would yield stronger features while cutting training time from 5 hours to under 30 minutes. Multimodal embeddings combining visual features with text descriptions could resolve ambiguity zones where visually similar but semantically unrelated items cluster together (e.g., conch shells with hamburgers based on warm colors and rounded shapes)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
