{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "373e093c",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This project explores an unsupervised clustering problem on the PatternMind dataset: 25k+ images spanning 233 fine-grained classes. Rather than working with raw pixels, we convert images into CNN feature vectors to analyze how they naturally group in this learned feature space. Ground-truth labels are used only to evaluate results and establish baselines. Our primary goal is to uncover the dataset's visual structure: which clusters emerge organically, how broader macro-groupings form, and where clustering reaches its limits.\n",
    "\n",
    "Our pipeline begins with data inspection (class imbalance, image dimensions), then trains a CNN primarily as a feature extractor, using its 256-D penultimate layer as a compact embedding for each image. We standardize these embeddings and apply PCA to reduce redundancy (50D for clustering, 2D for visualization). Hierarchical clustering on category centroids reveals data-driven macro-categories that capture visual affinities at multiple granularities (5–30 groups).\n",
    "\n",
    "We evaluate K-Means clustering by measuring purity against both the original 233 labels and our derived macro-categories. A supervised Logistic Regression classifier on the same embeddings provides an upper bound, quantifying the gap between unsupervised clustering and label classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66126d12",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Initial Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72c16aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras import regularizers\n",
    "from PIL import Image\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import plotly.figure_factory as ff\n",
    "from plotly.subplots import make_subplots\n",
    "import keras\n",
    "from collections import Counter\n",
    "from keras import layers\n",
    "import tensorflow as tf\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy.cluster.hierarchy import linkage, fcluster, leaves_list\n",
    "from scipy.spatial.distance import pdist, squareform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c9e724",
   "metadata": {},
   "source": [
    "### 1.1 Load Image Paths and Labels\n",
    "\n",
    "We load all images from the dataset directory, where each subfolder represents a distinct category. The code below creates a DataFrame containing the file path, category name, and numeric label for each image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1dcf35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"db/patternmind_dataset/\"\n",
    "\n",
    "# Get all category folders (each subfolder is a class)\n",
    "categories = sorted([d for d in os.listdir(dataset_path) if os.path.isdir(\n",
    "    os.path.join(dataset_path, d)) and not d.startswith('.')])\n",
    "\n",
    "# Build a list of all images with their metadata\n",
    "data = []\n",
    "for label_id, category in enumerate(categories):\n",
    "    folder_path = os.path.join(dataset_path, category)\n",
    "    images = [f for f in os.listdir(\n",
    "        folder_path) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "\n",
    "    for img in images:\n",
    "        data.append({\n",
    "            'path': os.path.join(folder_path, img),\n",
    "            'category': category,\n",
    "            'label_id': label_id\n",
    "        })\n",
    "\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2706874",
   "metadata": {},
   "source": [
    "### 1.2 Dataset Statistics and Quality Check\n",
    "\n",
    "The code below checks for missing values, duplicates and computes summary statistics about the class distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f0d822",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values and data quality\n",
    "print(f\"Missing values in DataFrame: {df.isnull().sum().sum()}\")\n",
    "print(f\"Duplicate rows: {df.duplicated().sum()}\")\n",
    "\n",
    "# Detailed statistics\n",
    "counts = df['category'].value_counts()\n",
    "print(f\"\\nTotal images: {len(df)}\")\n",
    "print(f\"Total categories: {len(categories)}\")\n",
    "print(f\"Average images per category: {len(df) / len(categories):.2f}\")\n",
    "print(f\"Min images per category: {counts.min()}\")\n",
    "print(f\"Max images per category: {counts.max()}\")\n",
    "print(f\"Median images per category: {counts.median()}\")\n",
    "print(f\"Std deviation: {counts.std():.2f}\")\n",
    "print(\"\\nEdge cases (fewest images):\")\n",
    "print(counts.tail())\n",
    "print(\"\\nEdge cases (most images):\")\n",
    "print(counts.head())\n",
    "\n",
    "# Class imbalance analysis\n",
    "imbalance_ratio = counts.max() / counts.min()\n",
    "print(f\"\\nClass imbalance ratio (max/min): {imbalance_ratio:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d809c3fd",
   "metadata": {},
   "source": [
    "The output above shows:\n",
    "\n",
    "- No missing values or duplicates\n",
    "- 25557 total images across 233 categories\n",
    "- Average of ~110 images per category, but with significant variation\n",
    "- Class imbalance ratio of 10.57x between the most and least frequent categories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d743e60",
   "metadata": {},
   "source": [
    "## 2. Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77826bf4",
   "metadata": {},
   "source": [
    "### 2.1 Class Distribution Analysis\n",
    "\n",
    "We first examine how images are distributed across categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b707408a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_dist = px.bar(counts, x=counts.index, y=counts.values,\n",
    "                  title=\"Class Distribution\",\n",
    "                  labels={'index': 'Category', 'y': 'Count'})\n",
    "fig_dist.update_layout(xaxis={'categoryorder': 'total descending'})\n",
    "fig_dist.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1088bbde",
   "metadata": {},
   "source": [
    "The bar chart reveals significant class imbalance in the dataset:\n",
    "\n",
    "- The most frequent category (\"clutter\") has 761 images\n",
    "- The least frequent category (\"top-hat\") has only 72 images\n",
    "- This represents a 10x variation in class sizes\n",
    "\n",
    "This imbalance matters for both supervised classification and unsupervised clustering. During CNN training we use data augmentation to help the model generalize across all categories regardless of frequency. For clustering we report purity, which is weighted by cluster size, so large clusters influence the score more than small ones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51ddac6",
   "metadata": {},
   "source": [
    "### 2.2 Image Dimension Analysis\n",
    "\n",
    "Before feeding images to a neural network, we need to understand the variation in image sizes. Neural networks require fixed-size inputs, so we must choose an appropriate target resolution. We analyze a sample of 5,000 images to understand the distribution of widths and heights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5fb2173",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample images to analyze dimension variability\n",
    "sample_size = 5000\n",
    "sample_df = df.sample(n=sample_size)\n",
    "sizes = [Image.open(p).size for p in sample_df['path']]\n",
    "widths, heights = zip(*sizes)\n",
    "\n",
    "# Create side-by-side histograms for width and height distributions\n",
    "fig_sizes = make_subplots(rows=1, cols=2, subplot_titles=(\n",
    "    \"Width Distribution\", \"Height Distribution\"))\n",
    "fig_sizes.add_trace(go.Histogram(x=widths, name=\"Width\"), row=1, col=1)\n",
    "fig_sizes.add_trace(go.Histogram(x=heights, name=\"Height\"), row=1, col=2)\n",
    "fig_sizes.update_layout(\n",
    "    title_text=f\"Image Size Distribution (Sample n={sample_size})\")\n",
    "fig_sizes.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c0beda",
   "metadata": {},
   "source": [
    "The histograms show that image dimensions vary considerably:\n",
    "\n",
    "- Most images have widths and heights concentrated around 200-400 pixels\n",
    "- Some images are very small (thumbnails) while others are much larger\n",
    "\n",
    "We chose 224×224 resolution (the ImageNet standard) because it is large enough to preserve important visual details like edges, textures, and shapes, while remaining computationally efficient by reducing memory usage and training time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f56c2ba",
   "metadata": {},
   "source": [
    "### 2.3 Visual Sample Inspection\n",
    "\n",
    "To understand the visual diversity in our dataset we display random samples from five different categories. This helps us appreciate the challenges our models will face: varying backgrounds, lighting conditions, object orientations and image quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c06867e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_categories = np.random.choice(categories, 5, replace=False)\n",
    "fig_imgs = make_subplots(rows=1, cols=5, subplot_titles=sample_categories)\n",
    "\n",
    "for i, cat in enumerate(sample_categories):\n",
    "    img_path = df[df['category'] == cat].sample(1).iloc[0]['path']\n",
    "    img = Image.open(img_path)\n",
    "    img_array = np.array(img)\n",
    "    fig_imgs.add_trace(go.Image(z=img_array), row=1, col=i+1)\n",
    "\n",
    "fig_imgs.update_layout(title_text=\"Sample Images from Random Categories\")\n",
    "fig_imgs.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597e09fe",
   "metadata": {},
   "source": [
    "The random samples reveal substantial visual diversity within and across categories.\n",
    "\n",
    "To address this variability, we use data augmentation (random flips, rotations, zoom, translations) so the model sees different orientations and scales of the same objects. The CNN’s stacked convolutional layers then learn hierarchical features that stay robust to these variations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac11f38b",
   "metadata": {},
   "source": [
    "## 3. CNN Feature Extraction\n",
    "\n",
    "We use a CNN for feature extraction because these networks learn representations at multiple levels of complexity. Early layers detect simple patterns like edges and textures while deeper layers recognize more complex features like shapes and objects. The layer just before the final classification output contains a rich and compressed representation of the image that captures its essential visual characteristics. These learned features work much better for clustering than using raw pixel values directly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23934a9",
   "metadata": {},
   "source": [
    "### 3.1 Data Preprocessing and Augmentation\n",
    "\n",
    "Before training the CNN, we standardize inputs and apply light augmentation:\n",
    "\n",
    "- Resize to 224×224: fixed input shape\n",
    "- Normalize to [0, 1]: scale pixels for faster convergence\n",
    "- Random horizontal flip: handle left/right orientation\n",
    "- Random rotation (±0.15 rad): cover tilt\n",
    "- Random zoom (±15%): cover scale changes\n",
    "- Random translation (±10% height/width): cover shifts\n",
    "\n",
    "We split 80% for training and 20% for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a4b165",
   "metadata": {},
   "outputs": [],
   "source": [
    "# type: ignore\n",
    "IMG_SIZE = (224, 224)\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Data augmentation: random flips, rotations, zoom, and translations\n",
    "data_augmentation = keras.Sequential([\n",
    "    layers.RandomFlip(\"horizontal\"),\n",
    "    layers.RandomRotation(0.15),\n",
    "    layers.RandomZoom(0.15),\n",
    "    layers.RandomTranslation(0.1, 0.1),\n",
    "])\n",
    "\n",
    "# Load training and validation datasets (80/20 split)\n",
    "train_ds = keras.utils.image_dataset_from_directory(\n",
    "    dataset_path,\n",
    "    validation_split=0.2,\n",
    "    subset=\"training\",\n",
    "    seed=123,\n",
    "    image_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    label_mode='categorical'\n",
    ")\n",
    "\n",
    "val_ds = keras.utils.image_dataset_from_directory(\n",
    "    dataset_path,\n",
    "    validation_split=0.2,\n",
    "    subset=\"validation\",\n",
    "    seed=123,\n",
    "    image_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    label_mode='categorical'\n",
    ")\n",
    "\n",
    "# Apply augmentation to training data and normalize both datasets\n",
    "train_ds = train_ds.map(lambda x, y: (x / 255.0, y))\n",
    "train_ds = train_ds.map(lambda x, y: (data_augmentation(x, training=True), y))\n",
    "train_ds = train_ds.cache().shuffle(1000).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "val_ds = val_ds.map(lambda x, y: (x / 255.0, y))\n",
    "val_ds = val_ds.cache().prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eecede11",
   "metadata": {},
   "source": [
    "### 3.2 CNN Model Architecture\n",
    "\n",
    "We design a custom CNN with three convolutional blocks to extract visual features from images.\n",
    "\n",
    "| Layer Type    | Details                                                              | Purpose                                        |\n",
    "| ------------- | -------------------------------------------------------------------- | ---------------------------------------------- |\n",
    "| Conv Block 1  | 32 filters × 2 layers, 3×3 kernel, SAME pad, L2=1e-4                 | Detect low-level features (edges, colors)      |\n",
    "| Conv Block 2  | 64 filters × 2 layers, 3×3 kernel, SAME pad, L2=1e-4 + Dropout 0.25  | Detect mid-level features (textures, patterns) |\n",
    "| Conv Block 3  | 128 filters × 2 layers, 3×3 kernel, SAME pad, L2=1e-4 + Dropout 0.25 | Detect high-level features (shapes, parts)     |\n",
    "| GlobalAvgPool |                                                                      | Aggregate spatial features                     |\n",
    "| Dense Layer   | 256 neurons, ReLU, L2=1e-4 + Dropout 0.5                             | Compact 256-D representation for clustering    |\n",
    "| Output Layer  | 233 neurons, softmax                                                 | Classify into one of 233 categories            |\n",
    "\n",
    "Key choices:\n",
    "\n",
    "- Activation: ReLU throughout convolutional and dense layers.\n",
    "- Padding: `padding=\"same\"` keeps spatial size, preserving edges.\n",
    "- Regularization: L2 weight decay `1e-4` on all Conv/Dense layers.\n",
    "- Pooling: MaxPooling halves spatial dimensions in each block; GlobalAveragePooling before Dense.\n",
    "- Dropout: 0.25 after pooling in blocks 2–3; 0.5 before the output layer.\n",
    "- Batch size: 32, balancing stability and memory.\n",
    "- Optimizer: Adam with learning rate 0.0005.\n",
    "- Loss: Categorical crossentropy.\n",
    "- Callbacks: Early stopping on `val_loss` (patience 5, restore best) and ReduceLROnPlateau (factor 0.5, patience 3, min_lr 1e-7).\n",
    "\n",
    "Our goal isn’t peak classification accuracy, we primarily use the 256-D Dense layer as the feature embedding for clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a48fbf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = 'out/models/cnn_feature_extractor_v3.keras'\n",
    "\n",
    "if os.path.exists(MODEL_PATH):\n",
    "    model = keras.models.load_model(MODEL_PATH)\n",
    "    print(\"Model loaded successfully!\")\n",
    "else:\n",
    "    # Build CNN with 3 convolutional blocks (32, 64, 128 filters)\n",
    "    model = keras.Sequential([\n",
    "        layers.Input(shape=(IMG_SIZE[0], IMG_SIZE[1], 3)),\n",
    "\n",
    "        # Block 1\n",
    "        layers.Conv2D(32, (3, 3), activation='relu', padding='same',\n",
    "                      kernel_regularizer=regularizers.l2(1e-4)),\n",
    "        layers.Conv2D(32, (3, 3), activation='relu', padding='same',\n",
    "                      kernel_regularizer=regularizers.l2(1e-4)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "\n",
    "        # Block 2\n",
    "        layers.Conv2D(64, (3, 3), activation='relu', padding='same',\n",
    "                      kernel_regularizer=regularizers.l2(1e-4)),\n",
    "        layers.Conv2D(64, (3, 3), activation='relu', padding='same',\n",
    "                      kernel_regularizer=regularizers.l2(1e-4)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Dropout(0.25),\n",
    "\n",
    "        # Block 3\n",
    "        layers.Conv2D(128, (3, 3), activation='relu', padding='same',\n",
    "                      kernel_regularizer=regularizers.l2(1e-4)),\n",
    "        layers.Conv2D(128, (3, 3), activation='relu', padding='same',\n",
    "                      kernel_regularizer=regularizers.l2(1e-4)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Dropout(0.25),\n",
    "\n",
    "        layers.GlobalAveragePooling2D(),\n",
    "\n",
    "        layers.Dense(256, activation='relu', name='features',\n",
    "                     kernel_regularizer=regularizers.l2(1e-4)),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(len(categories), activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=0.0005),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    # Training callbacks for early stopping and learning rate reduction\n",
    "    callbacks = [\n",
    "        keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=5,\n",
    "            restore_best_weights=True\n",
    "        ),\n",
    "        keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=3,\n",
    "            min_lr=1e-7\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    history = model.fit(\n",
    "        train_ds,\n",
    "        epochs=50,\n",
    "        validation_data=val_ds,\n",
    "        callbacks=callbacks\n",
    "    )\n",
    "\n",
    "    # Save the trained model\n",
    "    os.makedirs('out/models', exist_ok=True)\n",
    "    model.save(MODEL_PATH)\n",
    "    print(f\"Model saved to {MODEL_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45621f6a",
   "metadata": {},
   "source": [
    "| Epoch | Accuracy   | Loss      | Val Accuracy | Val Loss  | Learning Rate |\n",
    "| ----- | ---------- | --------- | ------------ | --------- | ------------- |\n",
    "| 1     | 7.09%      | 5.151     | 6.05%        | 5.081     | 0.00050       |\n",
    "| 5     | 14.71%     | 4.347     | 12.85%       | 4.558     | 0.00050       |\n",
    "| 10    | 20.88%     | 3.841     | 14.46%       | 4.479     | 0.00050       |\n",
    "| 15    | 26.52%     | 3.498     | 20.00%       | 3.997     | 0.00050       |\n",
    "| 20    | 32.46%     | 3.111     | 26.18%       | 3.656     | 0.00025       |\n",
    "| 25    | 35.62%     | 2.930     | 24.87%       | 3.726     | 0.00025       |\n",
    "| 30    | 38.10%     | 2.778     | 26.57%       | 3.689     | 0.00025       |\n",
    "| 33    | 40.91%     | 2.624     | 30.23%       | 3.409     | 0.000125      |\n",
    "| 37    | **43.06%** | **2.501** | **30.33%**   | **3.445** | **0.0000625** |\n",
    "| 38    | 43.26%     | 2.496     | 30.09%       | 3.470     | 0.0000625     |\n",
    "\n",
    "The model trained for 38 epochs total, with the final weights saved at the end. The ReduceLROnPlateau callback automatically cut the learning rate in half three times: first at epoch 20 (from 5e-4 to 2.5e-4), again at epoch 33 (to 1.25e-4), and finally at epoch 37 (to 6.25e-5). This gradual slowdown helped the model fine-tune its weights more carefully as training progressed.\n",
    "\n",
    "Looking at performance, the model hit its best validation accuracy of 30.3% at epoch 37 and its lowest validation loss of 3.41 at epoch 33. By the final epoch, training accuracy reached 43.3% while validation sat at 30.1%: a gap of about 13 percentage points. This gap shows the model is overfitting somewhat.\n",
    "\n",
    "That said, 30% validation accuracy on 233 categories isn't terrible (random guessing would only get 0.43%). More importantly, we're not really after high classification scores here. What matters is that the network learned useful visual patterns in those intermediate layers and the 256-dimensional feature embedding it produces is what we'll actually use for clustering downstream."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88bf5e9",
   "metadata": {},
   "source": [
    "### 3.3 Feature Extraction from All Images\n",
    "\n",
    "Now we extract 256-dimensional feature vectors from every image in the dataset. We do this by:\n",
    "\n",
    "1. Removing the final classification layer from the trained CNN\n",
    "2. Passing each image through the network\n",
    "3. Collecting the output from the Dense(256) layer\n",
    "\n",
    "These features capture the \"essence\" of each image as learned by the CNN: a compact representation that preserves visual similarity. Images that look alike will have similar feature vectors making them suitable for clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c838a480",
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES_PATH = 'out/features/features_v3.npy'\n",
    "LABELS_PATH = 'out/features/labels_v3.npy'\n",
    "\n",
    "if os.path.exists(FEATURES_PATH) and os.path.exists(LABELS_PATH):\n",
    "    features = np.load(FEATURES_PATH)\n",
    "    labels = np.load(LABELS_PATH)\n",
    "    print(f\"Features loaded: {features.shape}\")\n",
    "    print(f\"Labels loaded: {labels.shape}\")\n",
    "else:\n",
    "    # Create feature extractor by removing final classification layer\n",
    "    feature_extractor = keras.Sequential(model.layers[:-1])  # type: ignore\n",
    "\n",
    "    # Load all images without augmentation\n",
    "    all_ds = keras.utils.image_dataset_from_directory(\n",
    "        dataset_path,\n",
    "        label_mode='int',\n",
    "        shuffle=False,\n",
    "        image_size=IMG_SIZE,\n",
    "        batch_size=BATCH_SIZE\n",
    "    )\n",
    "\n",
    "    all_ds = all_ds.map(lambda x, y: (x / 255.0, y))  # type: ignore\n",
    "\n",
    "    # Extract 256-dimensional features for all images\n",
    "    features = feature_extractor.predict(all_ds.map(lambda x, y: x))\n",
    "    labels = np.concatenate([y.numpy() for _, y in all_ds], axis=0)\n",
    "\n",
    "    # Save features and labels\n",
    "    os.makedirs('out/features', exist_ok=True)\n",
    "    np.save(FEATURES_PATH, features)\n",
    "    np.save(LABELS_PATH, labels)\n",
    "    print(f\"Features extracted and saved:\")\n",
    "    print(f\"Features shape: {features.shape}\")\n",
    "    print(f\"Labels shape: {labels.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b95990",
   "metadata": {},
   "source": [
    "## 4. Dimensionality Reduction with PCA\n",
    "\n",
    "Our CNN outputs 256-dimensional feature vectors for each image, but working with that many dimensions has drawbacks. In high-dimensional spaces distances become less meaningful, clustering algorithms slow down and visualizations become impossible. Plus, the CNN's 256 features likely contain redundancy: some dimensions might encode similar information or just noise.\n",
    "\n",
    "Principal Component Analysis (PCA) helps by finding new axes (principal components) that capture the maximum variance in our data. We can then keep only the most important components, throwing away redundant or noisy dimensions while preserving the essential structure. This makes clustering faster and more effective and lets us create 2D visualizations to understand what the CNN learned."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e678a56c",
   "metadata": {},
   "source": [
    "### 4.1 Feature Standardization and Variance Analysis\n",
    "\n",
    "We standardize all 256 CNN features to zero mean and unit variance then fit PCA to analyze how variance is distributed across components. This tells us how many dimensions actually carry useful information versus redundancy or noise. The cumulative variance plot below guides our choice of how many components to retain for downstream clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c52561",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_loaded = np.load(FEATURES_PATH)\n",
    "labels_loaded = np.load(LABELS_PATH)\n",
    "\n",
    "# Standardize features (mean=0, std=1)\n",
    "scaler = StandardScaler()\n",
    "features_scaled = scaler.fit_transform(features_loaded)\n",
    "\n",
    "# Fit PCA to analyze variance distribution\n",
    "pca_full = PCA()\n",
    "pca_full.fit(features_scaled)\n",
    "\n",
    "explained_variance = pca_full.explained_variance_ratio_\n",
    "cumulative_variance = np.cumsum(explained_variance)\n",
    "\n",
    "fig_variance = go.Figure()\n",
    "fig_variance.add_trace(go.Scatter(\n",
    "    x=list(range(1, len(explained_variance) + 1)),\n",
    "    y=cumulative_variance,\n",
    "    mode='lines+markers',\n",
    "    name='Cumulative Variance'\n",
    "))\n",
    "fig_variance.update_layout(\n",
    "    title='PCA Explained Variance',\n",
    "    xaxis_title='Number of Components',\n",
    "    yaxis_title='Cumulative Explained Variance',\n",
    "    hovermode='x'\n",
    ")\n",
    "fig_variance.show()\n",
    "\n",
    "print(\n",
    "    f\"Variance explained by first 2 components: {cumulative_variance[1]:.2%}\")\n",
    "print(\n",
    "    f\"Variance explained by first 10 components: {cumulative_variance[9]:.2%}\")\n",
    "print(\n",
    "    f\"Variance explained by first 50 components: {cumulative_variance[49]:.2%}\")\n",
    "print(\n",
    "    f\"Variance explained by first 60 components: {cumulative_variance[59]:.2%}\")\n",
    "print(\n",
    "    f\"Variance explained by first 75 components: {cumulative_variance[74]:.2%}\")\n",
    "print(\n",
    "    f\"Variance explained by first 100 components: {cumulative_variance[99]:.2%}\")\n",
    "print(\n",
    "    f\"Variance explained by first 120 components: {cumulative_variance[119]:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797642da",
   "metadata": {},
   "source": [
    "| Components | Variance Explained |\n",
    "| ---------- | ------------------ |\n",
    "| First 2    | 15.82%             |\n",
    "| First 10   | 50.51%             |\n",
    "| First 50   | 85.06%             |\n",
    "| First 60   | 87.31%             |\n",
    "| First 75   | 89.88%             |\n",
    "| First 100  | 92.95%             |\n",
    "| First 120  | 94.71%             |\n",
    "\n",
    "The variance distribution shows that the CNN's 256-dimensional output contains substantial redundancy.\n",
    "\n",
    "- The first 10 components alone capture over half the variance (50.51%)\n",
    "- The first 50 components retain 85% of the information\n",
    "- Beyond 100 components, variance gains diminish significantly\n",
    "- The curve shows smooth accumulation without a sharp elbow, indicating that variance is distributed across many components rather than concentrated in a few\n",
    "\n",
    "We choose 50 components for downstream clustering. This configuration retains 85.06% of the total variance while reducing dimensionality by over 80% compared to the original 256 dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38bf9245",
   "metadata": {},
   "source": [
    "### 4.2 2D Visualization\n",
    "\n",
    "While 50 dimensions is optimal for clustering, we cannot visualize 50D data directly. We project features to just 2 dimensions (PC1 and PC2) to create scatter plots that give us intuition about the data structure. We visualize 5 categories selected for their semantic diversity: airplanes, saturn, mars, duck and goose. Note that this 2D projection captures only ~16% of the total variance, so some overlap is expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633bdb14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce to 50D for clustering\n",
    "pca_50d = PCA(n_components=50)\n",
    "features_50d = pca_50d.fit_transform(features_scaled)\n",
    "\n",
    "# Reduce to 2D for visualization\n",
    "pca_2d = PCA(n_components=2)\n",
    "features_2d = pca_2d.fit_transform(features_scaled)\n",
    "\n",
    "# Visualize semantically distinct categories\n",
    "distinct_categories = ['airplanes', 'saturn', 'mars', 'duck', 'goose']\n",
    "distinct_indices = [i for i, cat in enumerate(\n",
    "    categories) if cat in distinct_categories]\n",
    "\n",
    "mask = np.isin(labels_loaded, distinct_indices)\n",
    "df_pca_distinct = pd.DataFrame({\n",
    "    'PC1': features_2d[mask, 0],\n",
    "    'PC2': features_2d[mask, 1],\n",
    "    'category': [categories[label] for label in labels_loaded[mask]]\n",
    "})\n",
    "\n",
    "fig_pca_distinct = px.scatter(\n",
    "    df_pca_distinct,\n",
    "    x='PC1',\n",
    "    y='PC2',\n",
    "    color='category',\n",
    "    title='PCA 2D Projection - Semantically Distinct Categories',\n",
    "    opacity=0.7\n",
    ")\n",
    "fig_pca_distinct.update_traces(marker=dict(size=5))\n",
    "fig_pca_distinct.update_layout(width=1000, height=800)\n",
    "fig_pca_distinct.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb14b50",
   "metadata": {},
   "source": [
    "The scatter plot reveals clear separation between semantically distinct categories. Airplanes (blue) form a tight, well-defined cluster in the upper-left region, demonstrating that the CNN learned distinctive features for this category.\n",
    "\n",
    "The space-related categories show interesting behavior: mars (purple) and saturn (orange) occupy the center region and overlap substantially with each other. This makes sense visually since both are celestial bodies with similar color palettes (warm oranges/reds) and spherical shapes. The CNN's features capture this shared \"space object\" signature.\n",
    "\n",
    "Similarly, the bird categories duck (orange) and goose (green) cluster together in the right portion of the plot, reflecting their shared visual characteristics: feathers, beaks, similar body shapes, and often similar backgrounds (water, grass).\n",
    "\n",
    "The key insight is that even in just 2 dimensions (capturing only ~16% of variance), the CNN features successfully separate semantically different concepts (vehicles vs. space objects vs. birds) while grouping visually similar categories together. This confirms that the learned representations encode meaningful visual structure that hierarchical and K-Means clustering can exploit in the full 50-dimensional space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7798b56",
   "metadata": {},
   "source": [
    "### 4.3 Data-Driven Macro-Category Discovery\n",
    "\n",
    "Rather than imposing predefined semantic groupings (e.g., \"animals,\" \"vehicles\"), we let the CNN features reveal natural visual clusters. We apply hierarchical clustering to the 233 category centroids using Ward linkage, which minimizes within-cluster variance at each merge step. The resulting dendrogram shows how categories progressively combine as we relax similarity thresholds.\n",
    "\n",
    "Categories merging at low heights are visually similar according to the CNN's learned representations, while those merging only at high heights are fundamentally different. By cutting the dendrogram at various heights, we can create different numbers of macro-categories (from 5 broad groups to 30+ finer divisions) for use in our clustering evaluation in Section 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2056d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute centroids for each of the 233 categories\n",
    "# A centroid is the mean feature vector of all images in that category\n",
    "category_centroids = []\n",
    "for i, category in enumerate(categories):\n",
    "    mask = labels == i\n",
    "    centroid = features_50d[mask].mean(axis=0)\n",
    "    category_centroids.append(centroid)\n",
    "\n",
    "category_centroids = np.array(category_centroids)\n",
    "\n",
    "# Perform hierarchical clustering on category centroids using Ward linkage\n",
    "# Ward linkage minimizes within-cluster variance at each merge step\n",
    "linkage_matrix = linkage(category_centroids, method='ward')\n",
    "\n",
    "# Create dendrogram with Plotly\n",
    "color_threshold = 0.5 * max(linkage_matrix[:, 2])  # ~50% of max height\n",
    "\n",
    "fig_dendro_analysis = ff.create_dendrogram(\n",
    "    category_centroids,\n",
    "    labels=categories,\n",
    "    linkagefun=lambda x: linkage_matrix,\n",
    "    color_threshold=color_threshold\n",
    ")\n",
    "\n",
    "# Add horizontal line showing a suggested cut threshold\n",
    "fig_dendro_analysis.add_hline(\n",
    "    y=color_threshold,\n",
    "    line_dash=\"dash\",\n",
    "    line_color=\"red\",\n",
    "    annotation_text=f\"Cut threshold ({color_threshold:.1f})\",\n",
    "    annotation_position=\"top right\"\n",
    ")\n",
    "\n",
    "fig_dendro_analysis.update_layout(\n",
    "    title='Hierarchical Clustering Dendrogram of 233 Category Centroids<br>(Ward Linkage, 50D PCA Features)',\n",
    "    xaxis_title='Category',\n",
    "    yaxis_title='Ward Distance',\n",
    "    height=800,\n",
    "    width=1400,\n",
    "    xaxis={'tickangle': 90, 'tickfont': {'size': 6}}\n",
    ")\n",
    "\n",
    "fig_dendro_analysis.show()\n",
    "\n",
    "# Analyze the structure at different cut heights\n",
    "print(\"Macro-category counts at different cut heights:\")\n",
    "cut_heights = [0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "max_height = max(linkage_matrix[:, 2])\n",
    "\n",
    "for ratio in cut_heights:\n",
    "    cut_height = ratio * max_height\n",
    "    n_clusters = len(\n",
    "        set(fcluster(linkage_matrix, cut_height, criterion='distance')))\n",
    "    print(\n",
    "        f\"Cut at {ratio*100:.0f}% max height ({cut_height:.1f}): {n_clusters} clusters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110fd17a",
   "metadata": {},
   "source": [
    "### 4.4 Category Centroid Distance Heatmap\n",
    "\n",
    "To visualize how categories relate to each other in the 50D feature space we compute pairwise Euclidean distances between all 233 category centroids. The distance matrix is then reordered according to the hierarchical clustering leaf order, so that visually similar categories appear adjacent to each other. We display the first 100 categories to keep the heatmap readable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015c7645",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute pairwise Euclidean distances between all 233 category centroids\n",
    "centroid_distances = pdist(category_centroids, metric='euclidean')\n",
    "distance_matrix = squareform(centroid_distances)\n",
    "\n",
    "# Get the optimal leaf ordering from hierarchical clustering\n",
    "# This reorders categories so that similar ones are adjacent\n",
    "leaf_order = leaves_list(linkage_matrix)\n",
    "ordered_categories = [categories[i] for i in leaf_order]\n",
    "\n",
    "# Reorder the distance matrix according to clustering\n",
    "ordered_distance_matrix = distance_matrix[np.ix_(leaf_order, leaf_order)]\n",
    "\n",
    "# Show a zoomed version\n",
    "n_zoom = 100\n",
    "zoom_categories = ordered_categories[:n_zoom]\n",
    "\n",
    "fig_heatmap_zoom = go.Figure(data=go.Heatmap(\n",
    "    z=ordered_distance_matrix[:n_zoom, :n_zoom],\n",
    "    x=zoom_categories,\n",
    "    y=zoom_categories,\n",
    "    colorscale='Viridis',\n",
    "    reversescale=True,\n",
    "    colorbar=dict(title='Euclidean Distance'),\n",
    "    hovertemplate='%{x} ↔ %{y}<br>Distance: %{z:.3f}<extra></extra>'\n",
    "))\n",
    "\n",
    "fig_heatmap_zoom.update_layout(\n",
    "    title=f'Zoomed Heatmap: First {n_zoom} Categories (Clustered Order)',\n",
    "    xaxis=dict(tickangle=45, tickfont=dict(size=9)),\n",
    "    yaxis=dict(tickfont=dict(size=9), autorange='reversed'),\n",
    "    width=900,\n",
    "    height=800\n",
    ")\n",
    "\n",
    "fig_heatmap_zoom.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251501c3",
   "metadata": {},
   "source": [
    "The heatmap reveals the distance structure among the first 100 categories ordered by hierarchical clustering. The diagonal shows zero distance (yellow) as expected for self-comparisons. Several block structures are visible along the diagonal, indicating groups of categories with low inter-category distances (teal/green regions).\n",
    "\n",
    "Notable patterns include a cluster of electronic/household objects in the upper-left (ipod, breadmaker, photocopier, microwave) and another group of elongated objects mid-section (chopsticks, tuning-fork, eyeglasses). The bottom-right corner shows a distinct block containing architectural structures (skyscraper, minaret, pyramid) with relatively low distances to each other but high distances (purple) to most other categories.\n",
    "\n",
    "The predominant teal coloring across most of the matrix indicates moderate distances between categories, suggesting that while the CNN learned to distinguish broad visual concepts, many categories share enough visual features (shapes, textures, colors) to remain relatively close in feature space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd48be59",
   "metadata": {},
   "source": [
    "## 5. Supervised Baseline Classifier\n",
    "\n",
    "Before evaluating unsupervised clustering, we establish a supervised baseline to measure how well the CNN features support classification when ground-truth labels are available. This upper bound helps us contextualize the clustering results: the gap between supervised accuracy and unsupervised purity reveals how much information is lost when labels are unavailable.\n",
    "\n",
    "We use Logistic Regression because it's fast, interpretable, and effective with high-dimensional features. Unlike the CNN (which is a complex neural network), Logistic Regression learns simple linear decision boundaries in the 256-dimensional feature space. Strong performance here would confirm that the CNN successfully encoded discriminative visual information suitable for both classification and clustering tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9645c381",
   "metadata": {},
   "source": [
    "### 5.1 Train-Test Split with Stratification\n",
    "\n",
    "We split our data into 80% training and 20% test sets using stratified sampling. Stratification ensures that each category maintains the same proportion in both sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c96c758",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    features_scaled,\n",
    "    labels_loaded,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=labels_loaded\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {X_train.shape[0]}\")\n",
    "print(f\"Test set size: {X_test.shape[0]}\")\n",
    "print(f\"Feature dimensions: {X_train.shape[1]}\")\n",
    "print(f\"Number of classes: {len(np.unique(y_train))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd935e6",
   "metadata": {},
   "source": [
    "### 5.2 Train Logistic Regression Classifier\n",
    "\n",
    "We train multiple Logistic Regression configurations on the 256-dimensional CNN features to find the best balance between fitting the training data and generalizing to unseen examples:\n",
    "\n",
    "| Configuration    | Description                                                   |\n",
    "| ---------------- | ------------------------------------------------------------- |\n",
    "| Baseline (C=1.0) | Default regularization strength                               |\n",
    "| C=0.1            | Stronger L2 regularization to reduce overfitting              |\n",
    "| C=0.1 + balanced | Regularization with class weighting for imbalanced categories |\n",
    "| PCA-50D + C=0.1  | Regularization on dimensionality-reduced features             |\n",
    "\n",
    "Key settings:\n",
    "\n",
    "- max_iter=1000: Sufficient iterations for convergence with 233 classes\n",
    "- solver='lbfgs': Efficient optimization for multinomial classification\n",
    "- C parameter: Controls regularization strength (lower = stronger regularization)\n",
    "- n_jobs=-1: Parallel processing across all CPU cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac81b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# type: ignore\n",
    "\n",
    "# Baseline: default Logistic Regression\n",
    "lr_baseline = LogisticRegression(\n",
    "    max_iter=1000,\n",
    "    random_state=42,\n",
    "    solver='lbfgs',\n",
    "    n_jobs=-1\n",
    ")\n",
    "lr_baseline.fit(X_train, y_train)\n",
    "\n",
    "# Approach 1: Stronger regularization (C=0.1)\n",
    "lr_regularized = LogisticRegression(\n",
    "    max_iter=1000,\n",
    "    random_state=42,\n",
    "    solver='lbfgs',\n",
    "    C=0.1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "lr_regularized.fit(X_train, y_train)\n",
    "\n",
    "# Approach 2: Regularization + class balancing\n",
    "lr_balanced = LogisticRegression(\n",
    "    max_iter=1000,\n",
    "    random_state=42,\n",
    "    solver='lbfgs',\n",
    "    C=0.1,\n",
    "    class_weight='balanced',\n",
    "    n_jobs=-1\n",
    ")\n",
    "lr_balanced.fit(X_train, y_train)\n",
    "\n",
    "# Approach 3: PCA-reduced features (50D)\n",
    "X_train_pca, X_test_pca, y_train_pca, y_test_pca = train_test_split(\n",
    "    features_50d, labels_loaded, test_size=0.2, random_state=42, stratify=labels_loaded\n",
    ")\n",
    "lr_pca = LogisticRegression(\n",
    "    max_iter=1000,\n",
    "    random_state=42,\n",
    "    solver='lbfgs',\n",
    "    C=0.1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "lr_pca.fit(X_train_pca, y_train_pca)\n",
    "\n",
    "# Collect results\n",
    "results = []\n",
    "configs = [\n",
    "    (\"Baseline (C=1.0)\", lr_baseline, X_train, X_test, y_train, y_test),\n",
    "    (\"C=0.1\", lr_regularized, X_train, X_test, y_train, y_test),\n",
    "    (\"C=0.1 + balanced\", lr_balanced, X_train, X_test, y_train, y_test),\n",
    "    (\"PCA-50D + C=0.1\", lr_pca, X_train_pca, X_test_pca, y_train_pca, y_test_pca),\n",
    "]\n",
    "\n",
    "for name, model, X_tr, X_te, y_tr, y_te in configs:\n",
    "    train_acc = accuracy_score(y_tr, model.predict(X_tr))\n",
    "    test_acc = accuracy_score(y_te, model.predict(X_te))\n",
    "    y_pred = model.predict(X_te)\n",
    "    p_macro, r_macro, f1_macro, _ = precision_recall_fscore_support(\n",
    "        y_te, y_pred, average='macro', zero_division=0\n",
    "    )\n",
    "    p_weighted, r_weighted, f1_weighted, _ = precision_recall_fscore_support(\n",
    "        y_te, y_pred, average='weighted', zero_division=0\n",
    "    )\n",
    "    results.append({\n",
    "        'Configuration': name,\n",
    "        'Train Acc': train_acc,\n",
    "        'Test Acc': test_acc,\n",
    "        'Macro F1': f1_macro,\n",
    "        'Weighted F1': f1_weighted\n",
    "    })\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "print(df_results.to_string(index=False))\n",
    "\n",
    "# Select best model for downstream analysis\n",
    "best_idx = df_results['Test Acc'].idxmax()\n",
    "best_config = df_results.loc[best_idx, 'Configuration']\n",
    "\n",
    "# Use best model for predictions\n",
    "if 'PCA' in best_config:\n",
    "    lr_classifier = lr_pca\n",
    "    y_test_pred = lr_pca.predict(X_test_pca)\n",
    "    y_train_pred = lr_pca.predict(X_train_pca)\n",
    "    train_accuracy = accuracy_score(y_train_pca, y_train_pred)\n",
    "    test_accuracy = accuracy_score(y_test_pca, y_test_pred)\n",
    "elif 'balanced' in best_config:\n",
    "    lr_classifier = lr_balanced\n",
    "    y_test_pred = lr_balanced.predict(X_test)\n",
    "    y_train_pred = lr_balanced.predict(X_train)\n",
    "    train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "    test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "elif 'C=0.1' in best_config:\n",
    "    lr_classifier = lr_regularized\n",
    "    y_test_pred = lr_regularized.predict(X_test)\n",
    "    y_train_pred = lr_regularized.predict(X_train)\n",
    "    train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "    test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "else:\n",
    "    lr_classifier = lr_baseline\n",
    "    y_test_pred = lr_baseline.predict(X_test)\n",
    "    y_train_pred = lr_baseline.predict(X_train)\n",
    "    train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "    test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "# Final metrics for best model\n",
    "precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(\n",
    "    y_test if 'PCA' not in best_config else y_test_pca,\n",
    "    y_test_pred, average='macro', zero_division=0\n",
    ")\n",
    "precision_weighted, recall_weighted, f1_weighted, _ = precision_recall_fscore_support(\n",
    "    y_test if 'PCA' not in best_config else y_test_pca,\n",
    "    y_test_pred, average='weighted', zero_division=0\n",
    ")\n",
    "\n",
    "print(f\"\\nBest Model: {best_config}\")\n",
    "print(f\"Training Accuracy: {train_accuracy:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"\\nMacro-averaged metrics:\")\n",
    "print(f\"  Precision: {precision_macro:.4f}\")\n",
    "print(f\"  Recall: {recall_macro:.4f}\")\n",
    "print(f\"  F1-Score: {f1_macro:.4f}\")\n",
    "print(f\"\\nWeighted-averaged metrics:\")\n",
    "print(f\"  Precision: {precision_weighted:.4f}\")\n",
    "print(f\"  Recall: {recall_weighted:.4f}\")\n",
    "print(f\"  F1-Score: {f1_weighted:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666ee9c6",
   "metadata": {},
   "source": [
    "| Configuration    | Train Acc | Test Acc  | Gap       | Macro F1  | Weighted F1 |\n",
    "| ---------------- | --------- | --------- | --------- | --------- | ----------- |\n",
    "| Baseline (C=1.0) | 80.7%     | 37.7%     | 43.0%     | 31.6%     | 37.3%       |\n",
    "| **C=0.1**        | **63.8%** | **40.9%** | **22.9%** | **33.8%** | **39.4%**   |\n",
    "| C=0.1 + balanced | 63.9%     | 39.2%     | 24.7%     | 33.3%     | 38.7%       |\n",
    "| PCA-50D + C=0.1  | 46.7%     | 39.2%     | 7.5%      | 31.5%     | 37.1%       |\n",
    "\n",
    "Best Model: C=0.1 (strongest regularization on full 256D features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3cf9d6f",
   "metadata": {},
   "source": [
    "### 5.3 Per-Category Performance Analysis\n",
    "\n",
    "We examine how classification accuracy varies across individual categories. This reveals which types of images are easiest or hardest to classify, and whether performance correlates with category size or visual distinctiveness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77fb2149",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate per-category accuracy\n",
    "per_class_accuracy = []\n",
    "for i, category in enumerate(categories):\n",
    "    mask = y_test == i\n",
    "    if mask.sum() > 0:\n",
    "        class_acc = (y_test_pred[mask] == i).sum() / mask.sum()\n",
    "        per_class_accuracy.append({\n",
    "            'category': category,\n",
    "            'label_id': i,\n",
    "            'accuracy': class_acc,\n",
    "            'test_samples': mask.sum()\n",
    "        })\n",
    "\n",
    "df_class_acc = pd.DataFrame(per_class_accuracy)\n",
    "df_class_acc_sorted = df_class_acc.sort_values('accuracy', ascending=False)\n",
    "\n",
    "print(\"\\nTop 10 Best Performing Categories:\")\n",
    "print(df_class_acc_sorted.head(10)[\n",
    "      ['category', 'accuracy', 'test_samples']].to_string(index=False))\n",
    "\n",
    "print(\"\\nTop 10 Worst Performing Categories:\")\n",
    "print(df_class_acc_sorted.tail(10)[\n",
    "      ['category', 'accuracy', 'test_samples']].to_string(index=False))\n",
    "\n",
    "print(f\"\\nMean per-class accuracy: {df_class_acc['accuracy'].mean():.4f}\")\n",
    "print(f\"Median per-class accuracy: {df_class_acc['accuracy'].median():.4f}\")\n",
    "print(f\"Std per-class accuracy: {df_class_acc['accuracy'].std():.4f}\")\n",
    "\n",
    "fig_acc_dist = go.Figure()\n",
    "fig_acc_dist.add_trace(go.Histogram(\n",
    "    x=df_class_acc['accuracy'],\n",
    "    nbinsx=30,\n",
    "    name='Per-Class Accuracy'\n",
    "))\n",
    "fig_acc_dist.update_layout(\n",
    "    title='Distribution of Per-Class Accuracy',\n",
    "    xaxis_title='Accuracy',\n",
    "    yaxis_title='Number of Classes',\n",
    "    showlegend=False,\n",
    "    height=500\n",
    ")\n",
    "fig_acc_dist.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f8f739",
   "metadata": {},
   "source": [
    "| Best Categories | Accuracy | Test Samples | Worst Categories | Accuracy | Test Samples |\n",
    "| --------------- | -------- | ------------ | ---------------- | -------- | ------------ |\n",
    "| car-side        | 100.0%   | 21           | windmill         | 0%       | 17           |\n",
    "| leopards        | 100.0%   | 35           | tuning-fork      | 0%       | 18           |\n",
    "| faces-easy      | 98.7%    | 79           | rifle            | 0%       | 19           |\n",
    "| motorbikes      | 98.6%    | 144          | dog              | 0%       | 19           |\n",
    "| airplanes       | 97.9%    | 144          | mailbox          | 0%       | 17           |\n",
    "\n",
    "Most categories cluster around 20-40% accuracy while a small subset achieves 80%+ accuracy. Two categories (car-side, leopards) achieve perfect 100% classification and several others exceed 90% (faces-easy, motorbikes, airplanes, sunflower, mars). These high-performing categories share distinctive visual signatures: consistent shapes, unique textures, or characteristic compositions that make them easily separable in feature space.\n",
    "\n",
    "Categories that fail completely (0% accuracy) include everyday objects that appear in varied contexts (dog, goat, sneaker) or items with ambiguous visual features (tuning-fork, stirrups, spoon). These objects lack the consistent visual patterns needed for reliable classification.\n",
    "\n",
    "| Statistic                 | Value  |\n",
    "| ------------------------- | ------ |\n",
    "| Mean per-class accuracy   | 34.52% |\n",
    "| Median per-class accuracy | 30.43% |\n",
    "| Standard deviation        | 23.90% |\n",
    "\n",
    "The high standard deviation (23.9%) confirms dramatic performance variation across categories."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee96b7c0",
   "metadata": {},
   "source": [
    "### 5.4 Supervised Classification on Macro-Categories\n",
    "\n",
    "To further validate that the data-driven macro-categories represent meaningful visual groupings, we train a Logistic Regression classifier on macro-category labels instead of the original 233 fine-grained categories. If macro-categories capture coherent visual concepts, classification accuracy should improve significantly compared to the fine-grained baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d880d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Logistic Regression on macro-categories at different granularities\n",
    "macro_classification_results = []\n",
    "macro_granularities = [5, 10, 15, 20, 30]\n",
    "\n",
    "\n",
    "def get_macro_labels(linkage_matrix, n_macro, category_labels):\n",
    "    \"\"\"Map each image's category to its macro-category from dendrogram cut.\"\"\"\n",
    "    cat_to_macro = fcluster(linkage_matrix, n_macro, criterion='maxclust')\n",
    "    return np.array([cat_to_macro[cat] - 1 for cat in category_labels])\n",
    "\n",
    "\n",
    "for n_macro in macro_granularities:\n",
    "    # Get macro-category labels for all images\n",
    "    macro_labels_all = get_macro_labels(linkage_matrix, n_macro, labels_loaded)\n",
    "\n",
    "    # Split data with stratification on macro-labels\n",
    "    X_train_macro, X_test_macro, y_train_macro, y_test_macro = train_test_split(\n",
    "        features_scaled,\n",
    "        macro_labels_all,\n",
    "        test_size=0.2,\n",
    "        random_state=42,\n",
    "        stratify=macro_labels_all\n",
    "    )\n",
    "\n",
    "    # Train Logistic Regression with best config (C=0.1)\n",
    "    lr_macro = LogisticRegression(\n",
    "        max_iter=1000,\n",
    "        random_state=42,\n",
    "        solver='lbfgs',\n",
    "        C=0.1,  # Best configuration from Section 5.2\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    lr_macro.fit(X_train_macro, y_train_macro)\n",
    "\n",
    "    # Evaluate\n",
    "    train_acc = accuracy_score(y_train_macro, lr_macro.predict(X_train_macro))\n",
    "    test_acc = accuracy_score(y_test_macro, lr_macro.predict(X_test_macro))\n",
    "\n",
    "    macro_classification_results.append({\n",
    "        'n_macro_categories': n_macro,\n",
    "        'train_accuracy': train_acc,\n",
    "        'test_accuracy': test_acc,\n",
    "        'random_baseline': 1 / n_macro\n",
    "    })\n",
    "\n",
    "    print(\n",
    "        f\"Macro-categories: {n_macro:2d} | Train Acc: {train_acc:.4f} | Test Acc: {test_acc:.4f} | Random: {1/n_macro:.4f}\")\n",
    "\n",
    "df_macro_class = pd.DataFrame(macro_classification_results)\n",
    "\n",
    "# Visualize results\n",
    "fig_macro_class = go.Figure()\n",
    "fig_macro_class.add_trace(go.Scatter(\n",
    "    x=df_macro_class['n_macro_categories'],\n",
    "    y=df_macro_class['test_accuracy'],\n",
    "    mode='lines+markers',\n",
    "    name='Test Accuracy',\n",
    "    marker=dict(size=10)\n",
    "))\n",
    "fig_macro_class.add_trace(go.Scatter(\n",
    "    x=df_macro_class['n_macro_categories'],\n",
    "    y=df_macro_class['random_baseline'],\n",
    "    mode='lines+markers',\n",
    "    name='Random Baseline',\n",
    "    line=dict(dash='dash')\n",
    "))\n",
    "fig_macro_class.add_hline(\n",
    "    y=0.409,\n",
    "    line_dash=\"dot\",\n",
    "    line_color=\"red\",\n",
    "    annotation_text=\"Fine-grained (233 classes): 40.9%\",\n",
    "    annotation_position=\"top right\"\n",
    ")\n",
    "fig_macro_class.update_layout(\n",
    "    title='Logistic Regression Accuracy on Data-Driven Macro-Categories (C=0.1)',\n",
    "    xaxis_title='Number of Macro-Categories',\n",
    "    yaxis_title='Test Accuracy',\n",
    "    height=500,\n",
    "    width=800\n",
    ")\n",
    "fig_macro_class.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63e2e31",
   "metadata": {},
   "source": [
    "As we reduce the number of target macro-categories from 30 to 5 test accuracy increases from 54% to 72%, showing that the CNN features support progressively better classification as the task becomes less granular. Even with just 5 broad visual groups, the classifier achieves 72% accuracy demonstrating that hierarchical clustering successfully identified meaningful visual structure. At all granularities, test accuracy remains far above the random baseline confirming that the learned features encode genuine visual patterns rather than noise. Reducing to 5-10 macro-categories achieves higher accuracy, validating our hierarchical clustering approach and suggesting that for practical applications with limited labeled data, training classifiers on 5-15 macro-categories is more effective than attempting fine-grained 233-way classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fdb2e04",
   "metadata": {},
   "source": [
    "## 6. K-Means Clustering Analysis\n",
    "\n",
    "Having established data-driven macro-categories through hierarchical clustering (Section 4.3), we now evaluate how well K-Means clustering aligns with these visual groupings. K-Means partitions images into k clusters by minimizing within-cluster variance, assigning each image to the nearest centroid. We measure cluster quality using purity: the fraction of images in each cluster that belong to the majority class. By evaluating purity against both the original 233 fine-grained categories and our hierarchically-derived macro-categories (5-30 groups), we can assess whether K-Means captures meaningful visual structure at different levels of granularity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e377541b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run K-Means with different cluster counts and compute purity against macro-categories\n",
    "\n",
    "cluster_counts = [10, 20, 50, 100, 233]\n",
    "\n",
    "\n",
    "def calculate_purity(cluster_labels, true_labels):\n",
    "    correct = sum(\n",
    "        Counter(true_labels[cluster_labels == c]).most_common(1)[0][1]\n",
    "        for c in np.unique(cluster_labels)\n",
    "    )\n",
    "    return correct / len(cluster_labels)\n",
    "\n",
    "\n",
    "# Run K-Means and calculate purity for all configurations\n",
    "all_purity_results = []\n",
    "for k in cluster_counts:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    cluster_labels = kmeans.fit_predict(features_50d)\n",
    "    fine_purity = calculate_purity(cluster_labels, labels_loaded)\n",
    "\n",
    "    for n_macro in macro_granularities:\n",
    "        macro_labels = get_macro_labels(linkage_matrix, n_macro, labels_loaded)\n",
    "        macro_purity = calculate_purity(cluster_labels, macro_labels)\n",
    "        all_purity_results.append({\n",
    "            'n_kmeans_clusters': k,\n",
    "            'n_macro_categories': n_macro,\n",
    "            'fine_grained_purity': fine_purity,\n",
    "            'macro_purity': macro_purity\n",
    "        })\n",
    "\n",
    "df_purity = pd.DataFrame(all_purity_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b395d564",
   "metadata": {},
   "source": [
    "### 6.1 Purity Visualization\n",
    "\n",
    "We visualize K-Means purity across two dimensions: the number of clusters (K) and the granularity of target categories. The line chart shows how purity changes as we move from fine-grained (233 categories) to coarser macro-categories (5-30 groups), while the heatmap provides a comprehensive view of all K × granularity combinations. Higher purity values (green) indicate that clusters are more homogeneous with respect to the target labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a960fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Purity line chart: fine-grained vs macro-categories\n",
    "fig_purity = go.Figure()\n",
    "for k in cluster_counts:\n",
    "    subset = df_purity[df_purity['n_kmeans_clusters'] == k]\n",
    "    x_vals = [233] + list(subset['n_macro_categories'])\n",
    "    y_vals = [subset['fine_grained_purity'].iloc[0]] + \\\n",
    "        list(subset['macro_purity'])\n",
    "    fig_purity.add_trace(go.Scatter(\n",
    "        x=x_vals, y=y_vals, mode='lines+markers', name=f'K={k}',\n",
    "        hovertemplate='%{x} categories<br>Purity: %{y:.4f}<extra></extra>'\n",
    "    ))\n",
    "fig_purity.update_layout(\n",
    "    title='K-Means Purity: Fine-Grained (233) vs Data-Driven Macro-Categories',\n",
    "    xaxis_title='Number of Target Categories (log scale)', yaxis_title='Cluster Purity',\n",
    "    xaxis_type='log', xaxis=dict(tickvals=[5, 10, 15, 20, 30, 50, 100, 233]),\n",
    "    height=500, width=900, legend_title='K-Means Clusters'\n",
    ")\n",
    "fig_purity.show()\n",
    "\n",
    "# Purity heatmap\n",
    "pivot = df_purity.pivot(index='n_kmeans_clusters',\n",
    "                        columns='n_macro_categories', values='macro_purity')\n",
    "pivot[233] = df_purity.groupby('n_kmeans_clusters')[\n",
    "    'fine_grained_purity'].first()\n",
    "pivot = pivot.sort_index(axis=1, ascending=False)\n",
    "\n",
    "fig_heatmap = go.Figure(data=go.Heatmap(\n",
    "    z=pivot.values, x=[str(c) for c in pivot.columns], y=[f'K={k}' for k in pivot.index],\n",
    "    colorscale='RdYlGn', text=np.round(pivot.values, 3), texttemplate='%{text}',\n",
    "    textfont={'size': 11}, colorbar=dict(title='Purity')\n",
    "))\n",
    "fig_heatmap.update_layout(\n",
    "    title='Purity Heatmap: K-Means Clusters vs Number of Target Categories',\n",
    "    xaxis_title='Number of Target Categories (Macro → Fine-grained)',\n",
    "    yaxis_title='K-Means Clusters', height=400, width=800\n",
    ")\n",
    "fig_heatmap.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a18107",
   "metadata": {},
   "source": [
    "The visualizations reveal that K-Means clustering aligns well with broad visual groupings but struggles with fine-grained distinctions. At the fine-grained level (233 categories), purity ranges from just 10.6% (K=10) to 24.2% (K=233), confirming that unsupervised clustering cannot recover the original category labels. However, when evaluated against 5 macro-categories, all K values converge to 57-67% purity: a 3-6× improvement demonstrating that K-Means successfully captures the coarse visual structure discovered by hierarchical clustering.\n",
    "\n",
    "The heatmap reinforces this pattern: the diagonal gradient from red (bottom-left, fine-grained with few clusters) to green (top-right, coarse macro-categories with many clusters) shows that purity increases as we either add more clusters or reduce the number of target categories. Notably, K=233 achieves only 24.2% purity on fine-grained labels compared to the supervised Logistic Regression's 40.9% accuracy: a gap that highlights the difficulty of unsupervised category recovery.\n",
    "\n",
    "For practical applications, these results suggest that K-Means on CNN features provides a viable solution for broad categorization tasks (e.g., distinguishing animals from vehicles with ~65% purity at 5 macro-categories), while fine-grained classification (e.g., leopard vs. cheetah) requires supervised learning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
